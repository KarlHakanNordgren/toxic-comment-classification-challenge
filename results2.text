MAX_DOCUMENT_LENGTH = int(max_document_length/7)
MIN_FREQUENCY = 10
VOCABULARY_SIZE = len(vocabulary_processor.vocabulary_)
NUM_FILTERS = 10
KERNEL_SIZE = 1
FILTER_SHAPE = [KERNEL_SIZE, EMBEDDING_SIZE, NUM_FILTERS]
POOLING_SIZE = SEQUENCE_LENGTH - KERNEL_SIZE + 1
STRIDE = 1
NUM_HIDDEN_1 = int(NUM_FILTERS/2)
NUM_CLASSES = 2

EMBEDDING_SIZE = 50

train_loss 0.333555, development_loss 0.306562, current_step 100, took 4.00971
train_loss 0.269191, development_loss 0.296797, current_step 200, took 49.0388
train_loss 0.294905, development_loss 0.288521, current_step 300, took 14.1569
train_loss 0.34962, development_loss 0.279104, current_step 400, took 12.2418
train_loss 0.377331, development_loss 0.265336, current_step 500, took 11.4824
train_loss 0.195575, development_loss 0.248661, current_step 600, took 8.6897
train_loss 0.120533, development_loss 0.229447, current_step 700, took 8.65263
train_loss 0.222935, development_loss 0.212975, current_step 800, took 9.19015
train_loss 0.242483, development_loss 0.198044, current_step 900, took 7.69971
train_loss 0.152757, development_loss 0.185029, current_step 1000, took 8.98394
train_loss 0.0855937, development_loss 0.176606, current_step 1100, took 8.11932
train_loss 0.201223, development_loss 0.169881, current_step 1200, took 7.86171
train_loss 0.0567818, development_loss 0.164634, current_step 1300, took 7.78546
train_loss 0.123148, development_loss 0.16207, current_step 1400, took 9.772
train_loss 0.265444, development_loss 0.157689, current_step 1500, took 10.6105
train_loss 0.135432, development_loss 0.153639, current_step 1600, took 8.32237
train_loss 0.109525, development_loss 0.151111, current_step 1700, took 7.26464
train_loss 0.183809, development_loss 0.148468, current_step 1800, took 7.95299
train_loss 0.157222, development_loss 0.145244, current_step 1900, took 7.69548
train_loss 0.0779516, development_loss 0.143985, current_step 2000, took 7.65914
train_loss 0.0482665, development_loss 0.144027, current_step 2100, took 7.92089
train_loss 0.188474, development_loss 0.141488, current_step 2200, took 8.22003
train_loss 0.0905589, development_loss 0.141367, current_step 2300, took 7.57922
train_loss 0.160584, development_loss 0.139382, current_step 2400, took 8.86107
train_loss 0.143151, development_loss 0.138437, current_step 2500, took 8.58909
train_loss 0.370954, development_loss 0.137619, current_step 2600, took 8.35007
train_loss 0.215638, development_loss 0.137164, current_step 2700, took 8.3871
train_loss 0.140136, development_loss 0.137457, current_step 2800, took 8.03197
train_loss 0.0903456, development_loss 0.135638, current_step 2900, took 7.01485
train_loss 0.254868, development_loss 0.135112, current_step 3000, took 6.78517
train_loss 0.165145, development_loss 0.13453, current_step 3100, took 7.03654
train_loss 0.166392, development_loss 0.134953, current_step 3200, took 7.5021
train_loss 0.119864, development_loss 0.133791, current_step 3300, took 7.25729
train_loss 0.169742, development_loss 0.133253, current_step 3400, took 6.9855
train_loss 0.219043, development_loss 0.133796, current_step 3500, took 6.99127
train_loss 0.104265, development_loss 0.132838, current_step 3600, took 7.97725
train_loss 0.107707, development_loss 0.132115, current_step 3700, took 7.00529
train_loss 0.0809693, development_loss 0.13149, current_step 3800, took 6.96283
train_loss 0.0565034, development_loss 0.131438, current_step 3900, took 7.31928
train_loss 0.0579047, development_loss 0.131688, current_step 4000, took 7.10813
train_loss 0.107034, development_loss 0.130523, current_step 4100, took 7.77922
train_loss 0.102106, development_loss 0.131025, current_step 4200, took 7.34532
train_loss 0.103482, development_loss 0.129445, current_step 4300, took 7.33096
train_loss 0.0305441, development_loss 0.131011, current_step 4400, took 6.83895
train_loss 0.0969121, development_loss 0.128649, current_step 4500, took 7.43127
train_loss 0.0629653, development_loss 0.128419, current_step 4600, took 7.25866
train_loss 0.297664, development_loss 0.129469, current_step 4700, took 6.83113
train_loss 0.374255, development_loss 0.129902, current_step 4800, took 7.13125
train_loss 0.0861074, development_loss 0.128064, current_step 4900, took 7.11811
train_loss 0.113939, development_loss 0.127566, current_step 5000, took 6.9046
train_loss 0.182765, development_loss 0.126804, current_step 5100, took 6.8523
train_loss 0.110829, development_loss 0.126625, current_step 5200, took 7.07537
train_loss 0.309484, development_loss 0.126472, current_step 5300, took 7.5202
train_loss 0.252986, development_loss 0.125881, current_step 5400, took 7.36314
train_loss 0.181774, development_loss 0.126215, current_step 5500, took 7.45716
train_loss 0.177621, development_loss 0.125849, current_step 5600, took 7.92726
train_loss 0.115486, development_loss 0.125819, current_step 5700, took 7.0203
train_loss 0.110514, development_loss 0.126495, current_step 5800, took 7.63965
train_loss 0.125952, development_loss 0.126306, current_step 5900, took 8.98454
train_loss 0.100168, development_loss 0.126666, current_step 6000, took 10.3771
train_loss 0.0807801, development_loss 0.126011, current_step 6100, took 8.33105
train_loss 0.0384657, development_loss 0.126032, current_step 6200, took 8.31464
train_loss 0.0449154, development_loss 0.126107, current_step 6300, took 6.94014
train_loss 0.0626022, development_loss 0.125268, current_step 6400, took 7.29724
train_loss 0.0347162, development_loss 0.125942, current_step 6500, took 7.66352
train_loss 0.1952, development_loss 0.125691, current_step 6600, took 7.71322
train_loss 0.0479424, development_loss 0.125658, current_step 6700, took 7.89698
train_loss 0.137437, development_loss 0.127141, current_step 6800, took 8.16817
train_loss 0.0227272, development_loss 0.124089, current_step 6900, took 7.08602
train_loss 0.103704, development_loss 0.124369, current_step 7000, took 6.89601
train_loss 0.148094, development_loss 0.124161, current_step 7100, took 7.38834
train_loss 0.0429721, development_loss 0.124624, current_step 7200, took 8.27707
train_loss 0.120029, development_loss 0.123889, current_step 7300, took 9.1335
train_loss 0.0881132, development_loss 0.12408, current_step 7400, took 7.06443
train_loss 0.0969832, development_loss 0.124069, current_step 7500, took 8.05995
train_loss 0.0399017, development_loss 0.123696, current_step 7600, took 7.38436
train_loss 0.168313, development_loss 0.123185, current_step 7700, took 7.0271
train_loss 0.0642231, development_loss 0.124462, current_step 7800, took 7.54099
train_loss 0.109286, development_loss 0.123002, current_step 7900, took 7.32023
train_loss 0.0751378, development_loss 0.123927, current_step 8000, took 7.95524
train_loss 0.0874533, development_loss 0.122782, current_step 8100, took 7.3069
train_loss 0.104637, development_loss 0.125054, current_step 8200, took 7.13649
train_loss 0.0718631, development_loss 0.124157, current_step 8300, took 7.20269
train_loss 0.0552962, development_loss 0.12343, current_step 8400, took 7.94344
train_loss 0.0585225, development_loss 0.123136, current_step 8500, took 7.7222
train_loss 0.143088, development_loss 0.123765, current_step 8600, took 9.31912
train_loss 0.0970855, development_loss 0.12425, current_step 8700, took 7.88544
train_loss 0.306238, development_loss 0.125106, current_step 8800, took 6.8694
train_loss 0.0555917, development_loss 0.123638, current_step 8900, took 6.78043
train_loss 0.0492807, development_loss 0.123532, current_step 9000, took 7.19087
train_loss 0.314398, development_loss 0.124407, current_step 9100, took 7.64135
train_loss 0.0991897, development_loss 0.123257, current_step 9200, took 8.51364
train_loss 0.0966294, development_loss 0.123299, current_step 9300, took 7.45493
train_loss 0.0389805, development_loss 0.123147, current_step 9400, took 7.18741
train_loss 0.0360502, development_loss 0.124461, current_step 9500, took 7.12848
train_loss 0.126047, development_loss 0.122662, current_step 9600, took 6.89998
train_loss 0.0511646, development_loss 0.12318, current_step 9700, took 7.14459
train_loss 0.128741, development_loss 0.125792, current_step 9800, took 7.03207
train_loss 0.017861, development_loss 0.124554, current_step 9900, took 7.55036
train_loss 0.0265306, development_loss 0.123745, current_step 10000, took 8.95006
train_loss 0.127336, development_loss 0.123259, current_step 10100, took 9.16245
train_loss 0.0827645, development_loss 0.122766, current_step 10200, took 7.11281
train_loss 0.124044, development_loss 0.122619, current_step 10300, took 7.99933
train_loss 0.0390506, development_loss 0.122527, current_step 10400, took 6.85071
train_loss 0.083262, development_loss 0.122244, current_step 10500, took 7.32902
train_loss 0.233549, development_loss 0.122661, current_step 10600, took 7.3616
train_loss 0.0609427, development_loss 0.122744, current_step 10700, took 7.33846
train_loss 0.0723832, development_loss 0.123439, current_step 10800, took 9.63326
train_loss 0.0886111, development_loss 0.125043, current_step 10900, took 9.10025
train_loss 0.0562639, development_loss 0.123709, current_step 11000, took 6.93142
train_loss 0.059766, development_loss 0.124836, current_step 11100, took 8.07963
train_loss 0.169509, development_loss 0.123941, current_step 11200, took 7.61744
train_loss 0.0857117, development_loss 0.124443, current_step 11300, took 7.20486
train_loss 0.0880783, development_loss 0.124571, current_step 11400, took 8.33859
train_loss 0.0284383, development_loss 0.123836, current_step 11500, took 9.27781
train_loss 0.0220443, development_loss 0.123362, current_step 11600, took 8.17558
train_loss 0.0408083, development_loss 0.123017, current_step 11700, took 8.15646
train_loss 0.0658133, development_loss 0.124037, current_step 11800, took 7.20261
train_loss 0.187807, development_loss 0.126424, current_step 11900, took 7.19754
train_loss 0.0327366, development_loss 0.122039, current_step 12000, took 8.85596
train_loss 0.0414141, development_loss 0.12267, current_step 12100, took 8.38308
train_loss 0.164039, development_loss 0.122426, current_step 12200, took 11.0382
train_loss 0.0552018, development_loss 0.122779, current_step 12300, took 9.04754
train_loss 0.131544, development_loss 0.122139, current_step 12400, took 8.09171
train_loss 0.0723623, development_loss 0.122754, current_step 12500, took 7.54516
train_loss 0.0880059, development_loss 0.122538, current_step 12600, took 8.41425
train_loss 0.0494449, development_loss 0.122404, current_step 12700, took 8.22779
train_loss 0.0518263, development_loss 0.122594, current_step 12800, took 8.82602
train_loss 0.0687841, development_loss 0.123714, current_step 12900, took 8.63991
train_loss 0.0250823, development_loss 0.121641, current_step 13000, took 7.63711
train_loss 0.0909427, development_loss 0.121966, current_step 13100, took 7.24266
train_loss 0.0620306, development_loss 0.122666, current_step 13200, took 6.86865
train_loss 0.0910371, development_loss 0.126672, current_step 13300, took 6.6582
train_loss 0.038285, development_loss 0.123167, current_step 13400, took 6.76625
train_loss 0.0312622, development_loss 0.123336, current_step 13500, took 7.38638
train_loss 0.116345, development_loss 0.122938, current_step 13600, took 7.32416
train_loss 0.0633781, development_loss 0.12311, current_step 13700, took 6.9851
train_loss 0.0322158, development_loss 0.123754, current_step 13800, took 6.86747
train_loss 0.0366008, development_loss 0.12607, current_step 13900, took 7.30031
train_loss 0.0631954, development_loss 0.123599, current_step 14000, took 6.91552
train_loss 0.0664931, development_loss 0.124154, current_step 14100, took 8.39271
train_loss 0.0777672, development_loss 0.123479, current_step 14200, took 7.94622
train_loss 0.152744, development_loss 0.123968, current_step 14300, took 7.27069
train_loss 0.108157, development_loss 0.122024, current_step 14400, took 8.48438
train_loss 0.107784, development_loss 0.122169, current_step 14500, took 8.10484
train_loss 0.131566, development_loss 0.124195, current_step 14600, took 6.89234
train_loss 0.0603262, development_loss 0.122744, current_step 14700, took 7.16841
train_loss 0.136673, development_loss 0.122929, current_step 14800, took 7.45096
train_loss 0.0209073, development_loss 0.125225, current_step 14900, took 7.67501
train_loss 0.0561102, development_loss 0.123469, current_step 15000, took 9.16806
train_loss 0.13688, development_loss 0.125355, current_step 15100, took 7.13805
train_loss 0.0277744, development_loss 0.122869, current_step 15200, took 8.6609
train_loss 0.0576338, development_loss 0.123359, current_step 15300, took 9.82765
train_loss 0.0621782, development_loss 0.122659, current_step 15400, took 8.00707
train_loss 0.0397915, development_loss 0.122839, current_step 15500, took 8.35665
train_loss 0.0792113, development_loss 0.12299, current_step 15600, took 7.81672
train_loss 0.0727896, development_loss 0.12271, current_step 15700, took 7.09222
train_loss 0.0310029, development_loss 0.123, current_step 15800, took 6.92671
train_loss 0.0782266, development_loss 0.123422, current_step 15900, took 7.42141
train_loss 0.0414348, development_loss 0.125555, current_step 16000, took 9.26276
train_loss 0.0613716, development_loss 0.125186, current_step 16100, took 8.34036
train_loss 0.126796, development_loss 0.124645, current_step 16200, took 6.96035
train_loss 0.0620406, development_loss 0.123928, current_step 16300, took 6.6665
train_loss 0.0590228, development_loss 0.124731, current_step 16400, took 6.71295
train_loss 0.0328518, development_loss 0.125344, current_step 16500, took 6.99523
train_loss 0.0685363, development_loss 0.125684, current_step 16600, took 7.73206
train_loss 0.0114865, development_loss 0.124527, current_step 16700, took 7.78766
train_loss 0.0561457, development_loss 0.125021, current_step 16800, took 9.08332
train_loss 0.0887391, development_loss 0.123744, current_step 16900, took 8.19357
train_loss 0.0203349, development_loss 0.125855, current_step 17000, took 7.20792
train_loss 0.0681479, development_loss 0.123451, current_step 17100, took 6.74148
train_loss 0.0313909, development_loss 0.124201, current_step 17200, took 7.50922
train_loss 0.0853537, development_loss 0.125328, current_step 17300, took 7.73091
train_loss 0.138544, development_loss 0.124424, current_step 17400, took 6.80808
train_loss 0.0659742, development_loss 0.124073, current_step 17500, took 6.74137
train_loss 0.0619608, development_loss 0.125439, current_step 17600, took 6.95984
train_loss 0.0571792, development_loss 0.124961, current_step 17700, took 8.04043
train_loss 0.0599179, development_loss 0.125529, current_step 17800, took 8.22067
train_loss 0.0758696, development_loss 0.124258, current_step 17900, took 8.50739
train_loss 0.0634172, development_loss 0.124949, current_step 18000, took 8.05008
train_loss 0.121752, development_loss 0.125226, current_step 18100, took 8.33454
train_loss 0.0446841, development_loss 0.123849, current_step 18200, took 7.77798
train_loss 0.0742758, development_loss 0.124729, current_step 18300, took 10.4162
train_loss 0.107578, development_loss 0.128247, current_step 18400, took 11.5378
train_loss 0.174197, development_loss 0.125754, current_step 18500, took 11.3953
train_loss 0.156372, development_loss 0.126376, current_step 18600, took 6.8674
train_loss 0.0968155, development_loss 0.126507, current_step 18700, took 8.31672
train_loss 0.145769, development_loss 0.1253, current_step 18800, took 8.24845
train_loss 0.14819, development_loss 0.126771, current_step 18900, took 7.53859
train_loss 0.0377062, development_loss 0.13027, current_step 19000, took 7.19669
train_loss 0.0229397, development_loss 0.125749, current_step 19100, took 7.17152
train_loss 0.0556599, development_loss 0.127992, current_step 19200, took 7.01728
train_loss 0.029345, development_loss 0.126502, current_step 19300, took 7.02532
train_loss 0.0840419, development_loss 0.126428, current_step 19400, took 6.9602
train_loss 0.114958, development_loss 0.124827, current_step 19500, took 7.0944
train_loss 0.0474119, development_loss 0.124765, current_step 19600, took 7.14086
train_loss 0.0532034, development_loss 0.12694, current_step 19700, took 7.00015
train_loss 0.0320006, development_loss 0.125912, current_step 19800, took 6.97299
train_loss 0.0348175, development_loss 0.125809, current_step 19900, took 6.98141
train_loss 0.159468, development_loss 0.127504, current_step 20000, took 6.86444
train_loss 0.0357806, development_loss 0.126179, current_step 20100, took 7.01252
train_loss 0.0347243, development_loss 0.128614, current_step 20200, took 7.46171
train_loss 0.0745265, development_loss 0.126179, current_step 20300, took 10.1515
train_loss 0.0877063, development_loss 0.128256, current_step 20400, took 7.4927
