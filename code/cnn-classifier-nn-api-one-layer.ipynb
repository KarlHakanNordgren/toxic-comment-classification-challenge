{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "# https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## plan\n",
    "\n",
    "# 1. open dataset\n",
    "# 2. clean dataset\n",
    "# 3. munge dataset\n",
    "# 4. make test train\n",
    "# 5. make constants\n",
    "# 6. make placeholders\n",
    "# 7. define network\n",
    "# 8. define loss\n",
    "# 9. define train_op\n",
    "# 10. define development_op\n",
    "# 11. seesion: epochs, batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to ../output/tf-logs/run-20180127111344/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "######################################################\n",
    "## 0. admin\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "ROOT_LOG_DIRECTORY = \"../output/tf-logs\"\n",
    "LOG_DIRECTORY = \"{}/run-{}/\".format(ROOT_LOG_DIRECTORY, now)\n",
    "print(\"Writing to {}\\n\".format(LOG_DIRECTORY))\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "def make_idx_batches(data, batch_size):\n",
    "    idx_array = np.arange(data.shape[0])\n",
    "    n = len(idx_array)\n",
    "    res = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        res.append(idx_array[i:(i+batch_size)])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 1. open dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "response_df = pd.read_pickle(\"../output/response_df.pkl\")\n",
    "train_df = pd.read_pickle(\"../output/train_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 2. clean dataset\n",
    "## 3. munge dataset\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "def clean_string(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "comment_text_list = train_df['comment_text'].str.lower().tolist()\n",
    "comment_text_list_clean = [clean_string(x) for x in comment_text_list]\n",
    "\n",
    "toxic_list = train_df['toxic'].tolist()\n",
    "toxic_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in toxic_list])\n",
    "\n",
    "severe_toxic_list = train_df['severe_toxic'].tolist()\n",
    "severe_toxic_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in severe_toxic_list])\n",
    "\n",
    "obscene_list = train_df['obscene'].tolist()\n",
    "obscene_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in obscene_list])\n",
    "\n",
    "threat_list = train_df['threat'].tolist()\n",
    "threat_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in threat_list])\n",
    "\n",
    "insult_list = train_df['insult'].tolist()\n",
    "insult_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in insult_list])\n",
    "\n",
    "identity_hate_list = train_df['identity_hate'].tolist()\n",
    "identity_hate_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in identity_hate_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "max_document_length = max([len(x.split(\" \")) for x in comment_text_list_clean])\n",
    "MAX_DOCUMENT_LENGTH = int(max_document_length/7)\n",
    "MIN_FREQUENCY = 10\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "from tensorflow.contrib.learn import preprocessing\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "vocabulary_processor = preprocessing.VocabularyProcessor(\n",
    "    max_document_length = MAX_DOCUMENT_LENGTH,\n",
    "    min_frequency = MIN_FREQUENCY\n",
    ")\n",
    "\n",
    "x_comment_text = np.array(list(vocabulary_processor.fit_transform(comment_text_list_clean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 4. make test train\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "train_proportion = 0.8\n",
    "shuffled_idx = np.random.permutation(np.arange(len(x_comment_text)))\n",
    "\n",
    "train_idx = np.random.choice(shuffled_idx, size = int(train_proportion * len(shuffled_idx)), replace = False)\n",
    "development_idx = np.array([x for x in shuffled_idx if not x in train_idx])\n",
    "\n",
    "x_comment_text_train = x_comment_text[train_idx]\n",
    "\n",
    "toxic_labels_train = toxic_labels[train_idx]\n",
    "severe_toxic_labels_train = severe_toxic_labels[train_idx]\n",
    "obscene_labels_train = obscene_labels[train_idx]\n",
    "threat_labels_train = threat_labels[train_idx]\n",
    "insult_labels_train = insult_labels[train_idx]\n",
    "identity_hate_labels_train = identity_hate_labels[train_idx]\n",
    "\n",
    "x_comment_text_development = x_comment_text[development_idx]\n",
    "\n",
    "toxic_labels_development = toxic_labels[development_idx]\n",
    "severe_toxic_labels_development = severe_toxic_labels[development_idx]\n",
    "obscene_labels_development = obscene_labels[development_idx]\n",
    "threat_labels_development = threat_labels[development_idx]\n",
    "insult_labels_development = insult_labels[development_idx]\n",
    "identity_hate_labels_development = identity_hate_labels[development_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## constants\n",
    "\n",
    "SEQUENCE_LENGTH = MAX_DOCUMENT_LENGTH\n",
    "NUM_CLASSES = toxic_labels_train.shape[1]\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## 6. make placeholders\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    comment_text_placeholder = tf.placeholder(tf.int32, shape = (None, SEQUENCE_LENGTH))\n",
    "\n",
    "    toxic_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    severe_toxic_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    obscene_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    threat_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    insult_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    identity_hate_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## train parameters\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 100\n",
    "NUM_DISPLAY_STEPS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "VOCABULARY_SIZE = len(vocabulary_processor.vocabulary_)\n",
    "EMBEDDING_SIZE = 50\n",
    "NUM_FILTERS = 10\n",
    "KERNEL_SIZE = 1\n",
    "FILTER_SHAPE = [KERNEL_SIZE, EMBEDDING_SIZE, NUM_FILTERS]\n",
    "POOLING_SIZE = SEQUENCE_LENGTH - KERNEL_SIZE + 1\n",
    "STRIDE = 1\n",
    "NUM_HIDDEN_1 = int(NUM_FILTERS/2)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## 7. define network\n",
    "\n",
    "def make_hidden_1(inputs, name):\n",
    "     return tf.layers.dense(\n",
    "        inputs = inputs, \n",
    "        units = NUM_UNITS_HIDDEN_1,\n",
    "        name = name\n",
    "     )\n",
    "    \n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.variable_scope(\"network\", reuse = tf.AUTO_REUSE):\n",
    "\n",
    "        embedding_coefficients = tf.Variable(\n",
    "            initial_value = tf.random_uniform([VOCABULARY_SIZE, EMBEDDING_SIZE], -1.0, 1.0), \n",
    "            name = \"embedding_coefficients\"\n",
    "        )\n",
    "        word_embeddings = tf.nn.embedding_lookup(embedding_coefficients, comment_text_placeholder)\n",
    "        # shape: [batch, SEQUENCE_LENGTH, EMBEDDING_SIZE]\n",
    "\n",
    "        filter_1_coefficients = tf.Variable(\n",
    "            initial_value = tf.truncated_normal(FILTER_SHAPE, stddev = 0.1), \n",
    "            name = \"filter_1_coefficients\"\n",
    "        )\n",
    "        filter_1 = tf.nn.conv1d(\n",
    "            value = word_embeddings,\n",
    "            filters = filter_1_coefficients,\n",
    "            stride = STRIDE,\n",
    "            padding = \"VALID\",\n",
    "            name = \"filter_1\"\n",
    "        )\n",
    "\n",
    "        filter_1_bias = tf.Variable(tf.constant(0.1, shape = [NUM_FILTERS]), name = \"filter_1_bias\")\n",
    "        # initialiser?\n",
    "        convolution_1 = tf.nn.relu(tf.nn.bias_add(filter_1, filter_1_bias), name = \"convolution_1\")\n",
    "        \n",
    "        # print(convolution_1.shape) # [batch, SEQUENCE_LENGTH - KERNEL_SIZE + 1, NUM_FILTERS]\n",
    "        \n",
    "        max_pooling1d = tf.nn.pool(\n",
    "            input = convolution_1,\n",
    "            window_shape = [POOLING_SIZE],\n",
    "            pooling_type = \"MAX\",\n",
    "            strides = [1],\n",
    "            padding = 'VALID',\n",
    "            name = \"max_pooling1d\"\n",
    "        )\n",
    "        # print(max_pooling1d.shape) # [batch, 1, NUM_FILTERS]\n",
    "        \n",
    "        max_pooling1d_flattened = tf.contrib.layers.flatten(max_pooling1d)\n",
    "        # print(max_pooling1d_flattened.shape) # [batch, 1, NUM_FILTERS]\n",
    "        \n",
    "        #toxic_dense = tf.layers.dense(\n",
    "        #    inputs = max_pooling1d_flattened, \n",
    "        #    units = NUM_HIDDEN_1,\n",
    "        #    activation = tf.nn.relu\n",
    "        #)\n",
    "        \n",
    "        #toxic_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"toxic_dense\")\n",
    "        #severe_toxic_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"severe_toxic_dense\")\n",
    "        #obscene_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"obscene_dense\")\n",
    "        #threat_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"threat_dense\")\n",
    "        #insult_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"insult_dense\")\n",
    "        #identity_hate_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"identity_hate_dense\")\n",
    "\n",
    "        # initialiser?\n",
    "        toxic_logits =  tf.layers.dense(\n",
    "            inputs = max_pooling1d_flattened, \n",
    "            units = NUM_CLASSES,\n",
    "            name = \"toxic_logits\"\n",
    "        )\n",
    "        #severe_toxic_logits = make_logits(inputs = toxic_dense, name = \"severe_toxic_logits\")\n",
    "        #obscene_logits = make_logits(inputs = toxic_dense, name = \"obscene_logits\")\n",
    "        #threat_logits = make_logits(inputs = toxic_dense, name = \"threat_logits\")\n",
    "        #insult_logits = make_logits(inputs = toxic_dense, name = \"insult_logits\")\n",
    "        #identity_hate_logits = make_logits(inputs = toxic_dense, name = \"identity_hate_logits\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 8. define loss\n",
    "\n",
    "def make_single_loss(logits, placeholder, name):\n",
    "    sigmoid_cross_entropy_op = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = logits, \n",
    "            labels = placeholder\n",
    "        )\n",
    "    return tf.reduce_mean(sigmoid_cross_entropy_op, name = name)\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "\n",
    "        toxic_loss_op = make_single_loss(toxic_logits, toxic_placeholder, \"toxic_loss_op\")\n",
    "        #severe_toxic_loss_op = make_single_loss(severe_toxic_logits, severe_toxic_placeholder, \"severe_toxic_loss_op\")\n",
    "        #obscene_loss_op = make_single_loss(obscene_logits, obscene_placeholder, \"obscene_loss_op\")\n",
    "        #threat_loss_op = make_single_loss(threat_logits, threat_placeholder, \"threat_loss_op\")\n",
    "        #insult_loss_op = make_single_loss(insult_logits, insult_placeholder, \"insult_loss_op\")\n",
    "        #identity_hate_loss_op = make_single_loss(identity_hate_logits, identity_hate_placeholder, \"identity_hate_loss_op\")\n",
    "\n",
    "        loss_op = tf.reduce_mean([toxic_loss_op])\n",
    "                                  #, severe_toxic_loss_op, obscene_loss_op, \n",
    "        #                          threat_loss_op, insult_loss_op, identity_hate_loss_op])\n",
    "\n",
    "        toxic_loss_summary_op = tf.summary.scalar(\"toxic_loss_op\", toxic_loss_op)\n",
    "        #severe_toxic_loss_op_summary = tf.summary.scalar(\"severe_toxic_loss_op\", severe_toxic_loss_op)\n",
    "        #obscene_loss_op_summary = tf.summary.scalar(\"obscene_loss_op\", obscene_loss_op)\n",
    "        #threat_loss_op_summary = tf.summary.scalar(\"threat_loss_op\", threat_loss_op)\n",
    "        #insult_loss_op_summary = tf.summary.scalar(\"insult_loss_op\", insult_loss_op)\n",
    "        #identity_hate_loss_op_summary = tf.summary.scalar(\"identity_hate_loss_op\", identity_hate_loss_op)\n",
    "\n",
    "        loss_summary_op = tf.summary.scalar(\"loss_op\", loss_op)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name network/embedding_coefficients:0/grad/hist is illegal; using network/embedding_coefficients_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/embedding_coefficients:0/grad/sparsity is illegal; using network/embedding_coefficients_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name network/filter_1_coefficients:0/grad/hist is illegal; using network/filter_1_coefficients_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/filter_1_coefficients:0/grad/sparsity is illegal; using network/filter_1_coefficients_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name network/filter_1_bias:0/grad/hist is illegal; using network/filter_1_bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/filter_1_bias:0/grad/sparsity is illegal; using network/filter_1_bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name network/toxic_logits/kernel:0/grad/hist is illegal; using network/toxic_logits/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/toxic_logits/kernel:0/grad/sparsity is illegal; using network/toxic_logits/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name network/toxic_logits/bias:0/grad/hist is illegal; using network/toxic_logits/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/toxic_logits/bias:0/grad/sparsity is illegal; using network/toxic_logits/bias_0/grad/sparsity instead.\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "import time, os\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## 9. define train_op\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.variable_scope(\"train\", reuse = tf.AUTO_REUSE):\n",
    "\n",
    "        global_step = tf.Variable(0, name = \"global_step\", trainable = False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss_op)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step)\n",
    "        \n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([toxic_loss_summary_op, loss_summary_op, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(LOG_DIRECTORY, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, tf.get_default_graph())\n",
    "\n",
    "        # Dev summaries\n",
    "        development_summary_op = tf.summary.merge([toxic_loss_summary_op, loss_summary_op])\n",
    "        development_summary_dir = os.path.join(LOG_DIRECTORY, \"summaries\", \"development\")\n",
    "        development_summary_writer = tf.summary.FileWriter(development_summary_dir, tf.get_default_graph())\n",
    "    \n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.333555, development_loss 0.306562, current_step 100, took 4.00971\n",
      "train_loss 0.269191, development_loss 0.296797, current_step 200, took 49.0388\n",
      "train_loss 0.294905, development_loss 0.288521, current_step 300, took 14.1569\n",
      "train_loss 0.34962, development_loss 0.279104, current_step 400, took 12.2418\n",
      "train_loss 0.377331, development_loss 0.265336, current_step 500, took 11.4824\n",
      "train_loss 0.195575, development_loss 0.248661, current_step 600, took 8.6897\n",
      "train_loss 0.120533, development_loss 0.229447, current_step 700, took 8.65263\n",
      "train_loss 0.222935, development_loss 0.212975, current_step 800, took 9.19015\n",
      "train_loss 0.242483, development_loss 0.198044, current_step 900, took 7.69971\n",
      "train_loss 0.152757, development_loss 0.185029, current_step 1000, took 8.98394\n",
      "train_loss 0.0855937, development_loss 0.176606, current_step 1100, took 8.11932\n",
      "train_loss 0.201223, development_loss 0.169881, current_step 1200, took 7.86171\n",
      "train_loss 0.0567818, development_loss 0.164634, current_step 1300, took 7.78546\n",
      "train_loss 0.123148, development_loss 0.16207, current_step 1400, took 9.772\n",
      "train_loss 0.265444, development_loss 0.157689, current_step 1500, took 10.6105\n",
      "train_loss 0.135432, development_loss 0.153639, current_step 1600, took 8.32237\n",
      "train_loss 0.109525, development_loss 0.151111, current_step 1700, took 7.26464\n",
      "train_loss 0.183809, development_loss 0.148468, current_step 1800, took 7.95299\n",
      "train_loss 0.157222, development_loss 0.145244, current_step 1900, took 7.69548\n",
      "train_loss 0.0779516, development_loss 0.143985, current_step 2000, took 7.65914\n",
      "train_loss 0.0482665, development_loss 0.144027, current_step 2100, took 7.92089\n",
      "train_loss 0.188474, development_loss 0.141488, current_step 2200, took 8.22003\n",
      "train_loss 0.0905589, development_loss 0.141367, current_step 2300, took 7.57922\n",
      "train_loss 0.160584, development_loss 0.139382, current_step 2400, took 8.86107\n",
      "train_loss 0.143151, development_loss 0.138437, current_step 2500, took 8.58909\n",
      "train_loss 0.370954, development_loss 0.137619, current_step 2600, took 8.35007\n",
      "train_loss 0.215638, development_loss 0.137164, current_step 2700, took 8.3871\n",
      "train_loss 0.140136, development_loss 0.137457, current_step 2800, took 8.03197\n",
      "train_loss 0.0903456, development_loss 0.135638, current_step 2900, took 7.01485\n",
      "train_loss 0.254868, development_loss 0.135112, current_step 3000, took 6.78517\n",
      "train_loss 0.165145, development_loss 0.13453, current_step 3100, took 7.03654\n",
      "train_loss 0.166392, development_loss 0.134953, current_step 3200, took 7.5021\n",
      "train_loss 0.119864, development_loss 0.133791, current_step 3300, took 7.25729\n",
      "train_loss 0.169742, development_loss 0.133253, current_step 3400, took 6.9855\n",
      "train_loss 0.219043, development_loss 0.133796, current_step 3500, took 6.99127\n",
      "train_loss 0.104265, development_loss 0.132838, current_step 3600, took 7.97725\n",
      "train_loss 0.107707, development_loss 0.132115, current_step 3700, took 7.00529\n",
      "train_loss 0.0809693, development_loss 0.13149, current_step 3800, took 6.96283\n",
      "train_loss 0.0565034, development_loss 0.131438, current_step 3900, took 7.31928\n",
      "train_loss 0.0579047, development_loss 0.131688, current_step 4000, took 7.10813\n",
      "train_loss 0.107034, development_loss 0.130523, current_step 4100, took 7.77922\n",
      "train_loss 0.102106, development_loss 0.131025, current_step 4200, took 7.34532\n",
      "train_loss 0.103482, development_loss 0.129445, current_step 4300, took 7.33096\n",
      "train_loss 0.0305441, development_loss 0.131011, current_step 4400, took 6.83895\n",
      "train_loss 0.0969121, development_loss 0.128649, current_step 4500, took 7.43127\n",
      "train_loss 0.0629653, development_loss 0.128419, current_step 4600, took 7.25866\n",
      "train_loss 0.297664, development_loss 0.129469, current_step 4700, took 6.83113\n",
      "train_loss 0.374255, development_loss 0.129902, current_step 4800, took 7.13125\n",
      "train_loss 0.0861074, development_loss 0.128064, current_step 4900, took 7.11811\n",
      "train_loss 0.113939, development_loss 0.127566, current_step 5000, took 6.9046\n",
      "train_loss 0.182765, development_loss 0.126804, current_step 5100, took 6.8523\n",
      "train_loss 0.110829, development_loss 0.126625, current_step 5200, took 7.07537\n",
      "train_loss 0.309484, development_loss 0.126472, current_step 5300, took 7.5202\n",
      "train_loss 0.252986, development_loss 0.125881, current_step 5400, took 7.36314\n",
      "train_loss 0.181774, development_loss 0.126215, current_step 5500, took 7.45716\n",
      "train_loss 0.177621, development_loss 0.125849, current_step 5600, took 7.92726\n",
      "train_loss 0.115486, development_loss 0.125819, current_step 5700, took 7.0203\n",
      "train_loss 0.110514, development_loss 0.126495, current_step 5800, took 7.63965\n",
      "train_loss 0.125952, development_loss 0.126306, current_step 5900, took 8.98454\n",
      "train_loss 0.100168, development_loss 0.126666, current_step 6000, took 10.3771\n",
      "train_loss 0.0807801, development_loss 0.126011, current_step 6100, took 8.33105\n",
      "train_loss 0.0384657, development_loss 0.126032, current_step 6200, took 8.31464\n",
      "train_loss 0.0449154, development_loss 0.126107, current_step 6300, took 6.94014\n",
      "train_loss 0.0626022, development_loss 0.125268, current_step 6400, took 7.29724\n",
      "train_loss 0.0347162, development_loss 0.125942, current_step 6500, took 7.66352\n",
      "train_loss 0.1952, development_loss 0.125691, current_step 6600, took 7.71322\n",
      "train_loss 0.0479424, development_loss 0.125658, current_step 6700, took 7.89698\n",
      "train_loss 0.137437, development_loss 0.127141, current_step 6800, took 8.16817\n",
      "train_loss 0.0227272, development_loss 0.124089, current_step 6900, took 7.08602\n",
      "train_loss 0.103704, development_loss 0.124369, current_step 7000, took 6.89601\n",
      "train_loss 0.148094, development_loss 0.124161, current_step 7100, took 7.38834\n",
      "train_loss 0.0429721, development_loss 0.124624, current_step 7200, took 8.27707\n",
      "train_loss 0.120029, development_loss 0.123889, current_step 7300, took 9.1335\n",
      "train_loss 0.0881132, development_loss 0.12408, current_step 7400, took 7.06443\n",
      "train_loss 0.0969832, development_loss 0.124069, current_step 7500, took 8.05995\n",
      "train_loss 0.0399017, development_loss 0.123696, current_step 7600, took 7.38436\n",
      "train_loss 0.168313, development_loss 0.123185, current_step 7700, took 7.0271\n",
      "train_loss 0.0642231, development_loss 0.124462, current_step 7800, took 7.54099\n",
      "train_loss 0.109286, development_loss 0.123002, current_step 7900, took 7.32023\n",
      "train_loss 0.0751378, development_loss 0.123927, current_step 8000, took 7.95524\n",
      "train_loss 0.0874533, development_loss 0.122782, current_step 8100, took 7.3069\n",
      "train_loss 0.104637, development_loss 0.125054, current_step 8200, took 7.13649\n",
      "train_loss 0.0718631, development_loss 0.124157, current_step 8300, took 7.20269\n",
      "train_loss 0.0552962, development_loss 0.12343, current_step 8400, took 7.94344\n",
      "train_loss 0.0585225, development_loss 0.123136, current_step 8500, took 7.7222\n",
      "train_loss 0.143088, development_loss 0.123765, current_step 8600, took 9.31912\n",
      "train_loss 0.0970855, development_loss 0.12425, current_step 8700, took 7.88544\n",
      "train_loss 0.306238, development_loss 0.125106, current_step 8800, took 6.8694\n",
      "train_loss 0.0555917, development_loss 0.123638, current_step 8900, took 6.78043\n",
      "train_loss 0.0492807, development_loss 0.123532, current_step 9000, took 7.19087\n",
      "train_loss 0.314398, development_loss 0.124407, current_step 9100, took 7.64135\n",
      "train_loss 0.0991897, development_loss 0.123257, current_step 9200, took 8.51364\n",
      "train_loss 0.0966294, development_loss 0.123299, current_step 9300, took 7.45493\n",
      "train_loss 0.0389805, development_loss 0.123147, current_step 9400, took 7.18741\n",
      "train_loss 0.0360502, development_loss 0.124461, current_step 9500, took 7.12848\n",
      "train_loss 0.126047, development_loss 0.122662, current_step 9600, took 6.89998\n",
      "train_loss 0.0511646, development_loss 0.12318, current_step 9700, took 7.14459\n",
      "train_loss 0.128741, development_loss 0.125792, current_step 9800, took 7.03207\n",
      "train_loss 0.017861, development_loss 0.124554, current_step 9900, took 7.55036\n",
      "train_loss 0.0265306, development_loss 0.123745, current_step 10000, took 8.95006\n",
      "train_loss 0.127336, development_loss 0.123259, current_step 10100, took 9.16245\n",
      "train_loss 0.0827645, development_loss 0.122766, current_step 10200, took 7.11281\n",
      "train_loss 0.124044, development_loss 0.122619, current_step 10300, took 7.99933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.0390506, development_loss 0.122527, current_step 10400, took 6.85071\n",
      "train_loss 0.083262, development_loss 0.122244, current_step 10500, took 7.32902\n",
      "train_loss 0.233549, development_loss 0.122661, current_step 10600, took 7.3616\n",
      "train_loss 0.0609427, development_loss 0.122744, current_step 10700, took 7.33846\n",
      "train_loss 0.0723832, development_loss 0.123439, current_step 10800, took 9.63326\n",
      "train_loss 0.0886111, development_loss 0.125043, current_step 10900, took 9.10025\n",
      "train_loss 0.0562639, development_loss 0.123709, current_step 11000, took 6.93142\n",
      "train_loss 0.059766, development_loss 0.124836, current_step 11100, took 8.07963\n",
      "train_loss 0.169509, development_loss 0.123941, current_step 11200, took 7.61744\n",
      "train_loss 0.0857117, development_loss 0.124443, current_step 11300, took 7.20486\n",
      "train_loss 0.0880783, development_loss 0.124571, current_step 11400, took 8.33859\n",
      "train_loss 0.0284383, development_loss 0.123836, current_step 11500, took 9.27781\n",
      "train_loss 0.0220443, development_loss 0.123362, current_step 11600, took 8.17558\n",
      "train_loss 0.0408083, development_loss 0.123017, current_step 11700, took 8.15646\n",
      "train_loss 0.0658133, development_loss 0.124037, current_step 11800, took 7.20261\n",
      "train_loss 0.187807, development_loss 0.126424, current_step 11900, took 7.19754\n",
      "train_loss 0.0327366, development_loss 0.122039, current_step 12000, took 8.85596\n",
      "train_loss 0.0414141, development_loss 0.12267, current_step 12100, took 8.38308\n",
      "train_loss 0.164039, development_loss 0.122426, current_step 12200, took 11.0382\n",
      "train_loss 0.0552018, development_loss 0.122779, current_step 12300, took 9.04754\n",
      "train_loss 0.131544, development_loss 0.122139, current_step 12400, took 8.09171\n",
      "train_loss 0.0723623, development_loss 0.122754, current_step 12500, took 7.54516\n",
      "train_loss 0.0880059, development_loss 0.122538, current_step 12600, took 8.41425\n",
      "train_loss 0.0494449, development_loss 0.122404, current_step 12700, took 8.22779\n",
      "train_loss 0.0518263, development_loss 0.122594, current_step 12800, took 8.82602\n",
      "train_loss 0.0687841, development_loss 0.123714, current_step 12900, took 8.63991\n",
      "train_loss 0.0250823, development_loss 0.121641, current_step 13000, took 7.63711\n",
      "train_loss 0.0909427, development_loss 0.121966, current_step 13100, took 7.24266\n",
      "train_loss 0.0620306, development_loss 0.122666, current_step 13200, took 6.86865\n",
      "train_loss 0.0910371, development_loss 0.126672, current_step 13300, took 6.6582\n",
      "train_loss 0.038285, development_loss 0.123167, current_step 13400, took 6.76625\n",
      "train_loss 0.0312622, development_loss 0.123336, current_step 13500, took 7.38638\n",
      "train_loss 0.116345, development_loss 0.122938, current_step 13600, took 7.32416\n",
      "train_loss 0.0633781, development_loss 0.12311, current_step 13700, took 6.9851\n",
      "train_loss 0.0322158, development_loss 0.123754, current_step 13800, took 6.86747\n",
      "train_loss 0.0366008, development_loss 0.12607, current_step 13900, took 7.30031\n",
      "train_loss 0.0631954, development_loss 0.123599, current_step 14000, took 6.91552\n",
      "train_loss 0.0664931, development_loss 0.124154, current_step 14100, took 8.39271\n",
      "train_loss 0.0777672, development_loss 0.123479, current_step 14200, took 7.94622\n",
      "train_loss 0.152744, development_loss 0.123968, current_step 14300, took 7.27069\n",
      "train_loss 0.108157, development_loss 0.122024, current_step 14400, took 8.48438\n",
      "train_loss 0.107784, development_loss 0.122169, current_step 14500, took 8.10484\n",
      "train_loss 0.131566, development_loss 0.124195, current_step 14600, took 6.89234\n",
      "train_loss 0.0603262, development_loss 0.122744, current_step 14700, took 7.16841\n",
      "train_loss 0.136673, development_loss 0.122929, current_step 14800, took 7.45096\n",
      "train_loss 0.0209073, development_loss 0.125225, current_step 14900, took 7.67501\n",
      "train_loss 0.0561102, development_loss 0.123469, current_step 15000, took 9.16806\n",
      "train_loss 0.13688, development_loss 0.125355, current_step 15100, took 7.13805\n",
      "train_loss 0.0277744, development_loss 0.122869, current_step 15200, took 8.6609\n",
      "train_loss 0.0576338, development_loss 0.123359, current_step 15300, took 9.82765\n",
      "train_loss 0.0621782, development_loss 0.122659, current_step 15400, took 8.00707\n",
      "train_loss 0.0397915, development_loss 0.122839, current_step 15500, took 8.35665\n",
      "train_loss 0.0792113, development_loss 0.12299, current_step 15600, took 7.81672\n",
      "train_loss 0.0727896, development_loss 0.12271, current_step 15700, took 7.09222\n",
      "train_loss 0.0310029, development_loss 0.123, current_step 15800, took 6.92671\n",
      "train_loss 0.0782266, development_loss 0.123422, current_step 15900, took 7.42141\n",
      "train_loss 0.0414348, development_loss 0.125555, current_step 16000, took 9.26276\n",
      "train_loss 0.0613716, development_loss 0.125186, current_step 16100, took 8.34036\n",
      "train_loss 0.126796, development_loss 0.124645, current_step 16200, took 6.96035\n",
      "train_loss 0.0620406, development_loss 0.123928, current_step 16300, took 6.6665\n",
      "train_loss 0.0590228, development_loss 0.124731, current_step 16400, took 6.71295\n",
      "train_loss 0.0328518, development_loss 0.125344, current_step 16500, took 6.99523\n",
      "train_loss 0.0685363, development_loss 0.125684, current_step 16600, took 7.73206\n",
      "train_loss 0.0114865, development_loss 0.124527, current_step 16700, took 7.78766\n",
      "train_loss 0.0561457, development_loss 0.125021, current_step 16800, took 9.08332\n",
      "train_loss 0.0887391, development_loss 0.123744, current_step 16900, took 8.19357\n",
      "train_loss 0.0203349, development_loss 0.125855, current_step 17000, took 7.20792\n",
      "train_loss 0.0681479, development_loss 0.123451, current_step 17100, took 6.74148\n",
      "train_loss 0.0313909, development_loss 0.124201, current_step 17200, took 7.50922\n",
      "train_loss 0.0853537, development_loss 0.125328, current_step 17300, took 7.73091\n",
      "train_loss 0.138544, development_loss 0.124424, current_step 17400, took 6.80808\n",
      "train_loss 0.0659742, development_loss 0.124073, current_step 17500, took 6.74137\n",
      "train_loss 0.0619608, development_loss 0.125439, current_step 17600, took 6.95984\n",
      "train_loss 0.0571792, development_loss 0.124961, current_step 17700, took 8.04043\n",
      "train_loss 0.0599179, development_loss 0.125529, current_step 17800, took 8.22067\n",
      "train_loss 0.0758696, development_loss 0.124258, current_step 17900, took 8.50739\n",
      "train_loss 0.0634172, development_loss 0.124949, current_step 18000, took 8.05008\n",
      "train_loss 0.121752, development_loss 0.125226, current_step 18100, took 8.33454\n",
      "train_loss 0.0446841, development_loss 0.123849, current_step 18200, took 7.77798\n",
      "train_loss 0.0742758, development_loss 0.124729, current_step 18300, took 10.4162\n",
      "train_loss 0.107578, development_loss 0.128247, current_step 18400, took 11.5378\n",
      "train_loss 0.174197, development_loss 0.125754, current_step 18500, took 11.3953\n",
      "train_loss 0.156372, development_loss 0.126376, current_step 18600, took 6.8674\n",
      "train_loss 0.0968155, development_loss 0.126507, current_step 18700, took 8.31672\n",
      "train_loss 0.145769, development_loss 0.1253, current_step 18800, took 8.24845\n",
      "train_loss 0.14819, development_loss 0.126771, current_step 18900, took 7.53859\n",
      "train_loss 0.0377062, development_loss 0.13027, current_step 19000, took 7.19669\n",
      "train_loss 0.0229397, development_loss 0.125749, current_step 19100, took 7.17152\n",
      "train_loss 0.0556599, development_loss 0.127992, current_step 19200, took 7.01728\n",
      "train_loss 0.029345, development_loss 0.126502, current_step 19300, took 7.02532\n",
      "train_loss 0.0840419, development_loss 0.126428, current_step 19400, took 6.9602\n",
      "train_loss 0.114958, development_loss 0.124827, current_step 19500, took 7.0944\n",
      "train_loss 0.0474119, development_loss 0.124765, current_step 19600, took 7.14086\n",
      "train_loss 0.0532034, development_loss 0.12694, current_step 19700, took 7.00015\n",
      "train_loss 0.0320006, development_loss 0.125912, current_step 19800, took 6.97299\n",
      "train_loss 0.0348175, development_loss 0.125809, current_step 19900, took 6.98141\n",
      "train_loss 0.159468, development_loss 0.127504, current_step 20000, took 6.86444\n",
      "train_loss 0.0357806, development_loss 0.126179, current_step 20100, took 7.01252\n",
      "train_loss 0.0347243, development_loss 0.128614, current_step 20200, took 7.46171\n",
      "train_loss 0.0745265, development_loss 0.126179, current_step 20300, took 10.1515\n",
      "train_loss 0.0877063, development_loss 0.128256, current_step 20400, took 7.4927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.110299, development_loss 0.127041, current_step 20500, took 7.69723\n",
      "train_loss 0.17482, development_loss 0.126083, current_step 20600, took 7.76685\n",
      "train_loss 0.153302, development_loss 0.128197, current_step 20700, took 8.61365\n",
      "train_loss 0.164496, development_loss 0.125952, current_step 20800, took 9.03077\n",
      "train_loss 0.0179986, development_loss 0.126064, current_step 20900, took 9.39474\n",
      "train_loss 0.0736792, development_loss 0.126945, current_step 21000, took 7.5707\n",
      "train_loss 0.0607111, development_loss 0.130311, current_step 21100, took 7.02543\n",
      "train_loss 0.0315047, development_loss 0.128306, current_step 21200, took 7.03947\n",
      "train_loss 0.0709904, development_loss 0.127133, current_step 21300, took 7.1373\n",
      "train_loss 0.111335, development_loss 0.127741, current_step 21400, took 7.33971\n",
      "train_loss 0.064588, development_loss 0.127528, current_step 21500, took 11.9886\n",
      "train_loss 0.0178389, development_loss 0.12869, current_step 21600, took 8.70554\n",
      "train_loss 0.113009, development_loss 0.131072, current_step 21700, took 7.23658\n",
      "train_loss 0.0336413, development_loss 0.128046, current_step 21800, took 7.07086\n",
      "train_loss 0.0978159, development_loss 0.132142, current_step 21900, took 8.96015\n",
      "train_loss 0.0618018, development_loss 0.127697, current_step 22000, took 8.69859\n",
      "train_loss 0.0588404, development_loss 0.129108, current_step 22100, took 7.64065\n",
      "train_loss 0.202615, development_loss 0.127127, current_step 22200, took 8.89317\n",
      "train_loss 0.11594, development_loss 0.129533, current_step 22300, took 6.9072\n",
      "train_loss 0.0422403, development_loss 0.130083, current_step 22400, took 6.85773\n",
      "train_loss 0.0350347, development_loss 0.128204, current_step 22500, took 6.93814\n",
      "train_loss 0.113169, development_loss 0.128484, current_step 22600, took 7.10465\n",
      "train_loss 0.0441885, development_loss 0.130793, current_step 22700, took 8.08369\n",
      "train_loss 0.0100359, development_loss 0.129117, current_step 22800, took 9.8868\n",
      "train_loss 0.104362, development_loss 0.129591, current_step 22900, took 9.0029\n",
      "train_loss 0.130276, development_loss 0.127969, current_step 23000, took 10.8109\n",
      "train_loss 0.026442, development_loss 0.128163, current_step 23100, took 9.52627\n",
      "train_loss 0.0569413, development_loss 0.128948, current_step 23200, took 7.52384\n",
      "train_loss 0.24536, development_loss 0.127132, current_step 23300, took 8.56435\n",
      "train_loss 0.185272, development_loss 0.128058, current_step 23400, took 8.42956\n",
      "train_loss 0.125646, development_loss 0.130642, current_step 23500, took 7.01093\n",
      "train_loss 0.186153, development_loss 0.129499, current_step 23600, took 9.24282\n",
      "train_loss 0.0454611, development_loss 0.130926, current_step 23700, took 9.15528\n",
      "train_loss 0.0201985, development_loss 0.129889, current_step 23800, took 9.6532\n",
      "train_loss 0.134029, development_loss 0.129116, current_step 23900, took 8.61676\n",
      "train_loss 0.0360327, development_loss 0.131389, current_step 24000, took 8.16469\n",
      "train_loss 0.0720149, development_loss 0.13116, current_step 24100, took 9.39719\n",
      "train_loss 0.027087, development_loss 0.129883, current_step 24200, took 10.0013\n",
      "train_loss 0.00626773, development_loss 0.130479, current_step 24300, took 8.17317\n",
      "train_loss 0.0195608, development_loss 0.131141, current_step 24400, took 8.06878\n",
      "train_loss 0.135982, development_loss 0.129922, current_step 24500, took 8.36586\n",
      "train_loss 0.0501362, development_loss 0.128729, current_step 24600, took 8.0751\n",
      "train_loss 0.0869602, development_loss 0.128574, current_step 24700, took 8.55849\n",
      "train_loss 0.105706, development_loss 0.131013, current_step 24800, took 8.03647\n",
      "train_loss 0.0605169, development_loss 0.129094, current_step 24900, took 9.11066\n",
      "train_loss 0.0233532, development_loss 0.129365, current_step 25000, took 8.70558\n",
      "train_loss 0.207796, development_loss 0.13104, current_step 25100, took 7.40676\n",
      "train_loss 0.10924, development_loss 0.129965, current_step 25200, took 8.39299\n",
      "train_loss 0.0296231, development_loss 0.133454, current_step 25300, took 6.86079\n",
      "train_loss 0.0387885, development_loss 0.129989, current_step 25400, took 7.15481\n",
      "train_loss 0.183806, development_loss 0.132, current_step 25500, took 8.25194\n",
      "train_loss 0.0248853, development_loss 0.13032, current_step 25600, took 7.47973\n",
      "train_loss 0.0488131, development_loss 0.129676, current_step 25700, took 6.94999\n",
      "train_loss 0.0415039, development_loss 0.130707, current_step 25800, took 9.27817\n",
      "train_loss 0.175138, development_loss 0.1294, current_step 25900, took 9.29172\n",
      "train_loss 0.0135187, development_loss 0.129507, current_step 26000, took 7.37925\n",
      "train_loss 0.0918696, development_loss 0.130207, current_step 26100, took 7.66053\n",
      "train_loss 0.114215, development_loss 0.133587, current_step 26200, took 7.01869\n",
      "train_loss 0.0931927, development_loss 0.130984, current_step 26300, took 7.9491\n",
      "train_loss 0.04206, development_loss 0.130625, current_step 26400, took 7.95968\n",
      "train_loss 0.0269854, development_loss 0.13115, current_step 26500, took 7.41746\n",
      "train_loss 0.093422, development_loss 0.130262, current_step 26600, took 7.66556\n",
      "train_loss 0.0949572, development_loss 0.131413, current_step 26700, took 8.31964\n",
      "train_loss 0.0569376, development_loss 0.13616, current_step 26800, took 7.4778\n",
      "train_loss 0.0598908, development_loss 0.131113, current_step 26900, took 7.08105\n",
      "train_loss 0.0937373, development_loss 0.137227, current_step 27000, took 7.22771\n",
      "train_loss 0.154912, development_loss 0.132419, current_step 27100, took 7.41889\n",
      "train_loss 0.0881037, development_loss 0.134661, current_step 27200, took 7.48194\n",
      "train_loss 0.0430338, development_loss 0.132022, current_step 27300, took 8.16721\n",
      "train_loss 0.0426077, development_loss 0.13466, current_step 27400, took 12.8093\n",
      "train_loss 0.130475, development_loss 0.135365, current_step 27500, took 8.30199\n",
      "train_loss 0.0473652, development_loss 0.13201, current_step 27600, took 8.13022\n",
      "train_loss 0.0823203, development_loss 0.132546, current_step 27700, took 11.7712\n",
      "train_loss 0.0286167, development_loss 0.13532, current_step 27800, took 8.68011\n",
      "train_loss 0.172037, development_loss 0.133097, current_step 27900, took 8.39814\n",
      "train_loss 0.0788988, development_loss 0.133098, current_step 28000, took 9.3111\n",
      "train_loss 0.0447076, development_loss 0.132643, current_step 28100, took 7.77471\n",
      "train_loss 0.0646996, development_loss 0.132439, current_step 28200, took 9.22643\n",
      "train_loss 0.0124994, development_loss 0.131771, current_step 28300, took 8.37091\n",
      "train_loss 0.0643275, development_loss 0.130879, current_step 28400, took 8.36243\n",
      "train_loss 0.0355485, development_loss 0.131888, current_step 28500, took 9.47391\n",
      "train_loss 0.0169995, development_loss 0.132099, current_step 28600, took 8.20404\n",
      "train_loss 0.107221, development_loss 0.133309, current_step 28700, took 11.3407\n",
      "train_loss 0.0629079, development_loss 0.137217, current_step 28800, took 7.39934\n",
      "train_loss 0.036503, development_loss 0.133507, current_step 28900, took 9.4346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-b500f190d9df>\", line 49, in <module>\n",
      "    _, train_loss, train_summary = session.run([train_op, loss_op, train_summary_op], feed_dict = train_feed_dict)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\n",
      "    options, run_metadata)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/conda/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/lib/python3.6/inspect.py\", line 732, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## session\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        start_of_step_time = default_timer()\n",
    "        \n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "            idx_batches = make_idx_batches(x_comment_text_train, BATCH_SIZE)\n",
    "            \n",
    "            development_loss_last = 100\n",
    "\n",
    "            for idx_batch in idx_batches:\n",
    "\n",
    "                x_comment_text_batch = x_comment_text_train[idx_batch]\n",
    "\n",
    "                toxic_labels_batch = toxic_labels_train[idx_batch]\n",
    "                #severe_toxic_labels_batch = severe_toxic_labels_train[idx_batch]\n",
    "                #obscene_labels_batch = obscene_labels_train[idx_batch]\n",
    "                #threat_labels_batch = threat_labels_train[idx_batch]\n",
    "                #insult_labels_batch = insult_labels_train[idx_batch]\n",
    "                #identity_hate_labels_batch = identity_hate_labels_train[idx_batch]\n",
    "\n",
    "                # train\n",
    "                train_feed_dict = {\n",
    "                    comment_text_placeholder: x_comment_text_batch,\n",
    "                    toxic_placeholder: toxic_labels_batch,\n",
    "                 #   severe_toxic_placeholder: severe_toxic_labels_batch, \n",
    "                 #   obscene_placeholder: obscene_labels_batch,\n",
    "                 #   threat_placeholder: threat_labels_batch,\n",
    "                 #   insult_placeholder: insult_labels_batch,\n",
    "                 #   identity_hate_placeholder: identity_hate_labels_batch\n",
    "                }\n",
    "\n",
    "                _, train_loss, train_summary = session.run([train_op, loss_op, train_summary_op], feed_dict = train_feed_dict)\n",
    "\n",
    "                current_step = tf.train.global_step(session, global_step)\n",
    "                \n",
    "                #train_summary_writer.add_summary(train_summary, current_step)\n",
    "\n",
    "                if current_step % NUM_DISPLAY_STEPS == 0:\n",
    "                    \n",
    "                    took = default_timer() - start_of_step_time\n",
    "                    start_of_step_time = default_timer()\n",
    "\n",
    "                    development_feed_dict = {\n",
    "                        comment_text_placeholder: x_comment_text_development,\n",
    "                        toxic_placeholder: toxic_labels_development,\n",
    "                    #    severe_toxic_placeholder: severe_toxic_labels_development, \n",
    "                    #    obscene_placeholder: obscene_labels_development,\n",
    "                    #    threat_placeholder: threat_labels_development,\n",
    "                    #    insult_placeholder: insult_labels_development,\n",
    "                    #    identity_hate_placeholder: identity_hate_labels_development\n",
    "                    }\n",
    "\n",
    "                    development_loss, development_summary = session.run(\n",
    "                        fetches = [loss_op, development_summary_op], \n",
    "                        feed_dict = development_feed_dict\n",
    "                    )\n",
    "                    \n",
    "                    print(\"train_loss {:g}, development_loss {:g}, current_step {:g}, took {:g}\".format(\n",
    "                        train_loss, development_loss, current_step, took))\n",
    "                        \n",
    "                    if development_loss - development_loss_last > 0:\n",
    "                        if patience_counter > 10: \n",
    "                            break\n",
    "                        patience_counter = patience_counter + 1\n",
    "                    else:\n",
    "                        patience_counter = 0\n",
    "                        \n",
    "                    development_loss_last = development_loss\n",
    "                    \n",
    "                    #development_summary_writer.add_summary(development_summary, current_step)\n",
    "                    \n",
    "            if development_loss - development_loss_last > 0: \n",
    "                break\n",
    "                    \n",
    "                    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
