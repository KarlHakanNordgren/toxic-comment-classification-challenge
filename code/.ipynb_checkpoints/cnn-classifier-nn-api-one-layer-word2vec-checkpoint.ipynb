{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "# https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py\n",
    "# https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec.py\n",
    "\n",
    "# word2vec: https://ireneli.eu/2017/01/17/tensorflow-07-word-embeddings-2-loading-pre-trained-vectors/\n",
    "\n",
    "# http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf\n",
    "# tweet tokeniser\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## plan\n",
    "\n",
    "# 1. open dataset\n",
    "# 2. clean dataset\n",
    "# 3. munge dataset\n",
    "# 4. make test train\n",
    "# 5. make constants\n",
    "# 6. make placeholders\n",
    "# 7. define network\n",
    "# 8. define loss\n",
    "# 9. define train_op\n",
    "# 10. define development_op\n",
    "# 11. seesion: epochs, batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-83b146c01b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded GloVe!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadGloVe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-83b146c01b3e>\u001b[0m in \u001b[0;36mloadGloVe\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0membd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "filename = '../embeddings/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "def loadGloVe(filename):\n",
    "    vocabulary = []\n",
    "    embd = []\n",
    "    with open(filename, 'r', errors = 'replace') as file:\n",
    "        for line in file.readlines():\n",
    "            row = line.strip().split(' ')\n",
    "            vocabulary.append(row[0])\n",
    "            embd.append(row[1:])\n",
    "        print('Loaded GloVe!')\n",
    "    return vocabulary, embd\n",
    "vocabulary, embd = loadGloVe(filename)\n",
    "\n",
    "VOCABULARY_SIZE = len(vocab)\n",
    "\n",
    "EMBEDDING_SIZE = len(embd[0])\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to ../output/tf-logs/run-20180127111344/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "######################################################\n",
    "## 0. admin\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "ROOT_LOG_DIRECTORY = \"../output/tf-logs\"\n",
    "LOG_DIRECTORY = \"{}/run-{}/\".format(ROOT_LOG_DIRECTORY, now)\n",
    "print(\"Writing to {}\\n\".format(LOG_DIRECTORY))\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "def make_idx_batches(data, batch_size):\n",
    "    idx_array = np.arange(data.shape[0])\n",
    "    n = len(idx_array)\n",
    "    res = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        res.append(idx_array[i:(i+batch_size)])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 1. open dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "response_df = pd.read_pickle(\"../output/response_df.pkl\")\n",
    "train_df = pd.read_pickle(\"../output/train_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 2. clean dataset\n",
    "## 3. munge dataset\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "def clean_string(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "comment_text_list = train_df['comment_text'].str.lower().tolist()\n",
    "comment_text_list_clean = [clean_string(x) for x in comment_text_list]\n",
    "\n",
    "toxic_list = train_df['toxic'].tolist()\n",
    "toxic_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in toxic_list])\n",
    "\n",
    "severe_toxic_list = train_df['severe_toxic'].tolist()\n",
    "severe_toxic_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in severe_toxic_list])\n",
    "\n",
    "obscene_list = train_df['obscene'].tolist()\n",
    "obscene_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in obscene_list])\n",
    "\n",
    "threat_list = train_df['threat'].tolist()\n",
    "threat_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in threat_list])\n",
    "\n",
    "insult_list = train_df['insult'].tolist()\n",
    "insult_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in insult_list])\n",
    "\n",
    "identity_hate_list = train_df['identity_hate'].tolist()\n",
    "identity_hate_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in identity_hate_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "max_document_length = max([len(x.split(\" \")) for x in comment_text_list_clean])\n",
    "MAX_DOCUMENT_LENGTH = int(max_document_length/10)\n",
    "MIN_FREQUENCY = 10\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "from tensorflow.contrib.learn import preprocessing\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "#init vocab processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "#fit the vocab from glove\n",
    "pretrain = vocab_processor.fit(vocab)\n",
    "#transform inputs\n",
    "x = np.array(list(vocab_processor.transform(your_raw_input)))\n",
    "\n",
    "vocabulary_processor = preprocessing.VocabularyProcessor(\n",
    "    max_document_length = MAX_DOCUMENT_LENGTH,\n",
    "    min_frequency = MIN_FREQUENCY\n",
    ")\n",
    "vocabulary_processor_pretrained = vocabulary_processor.fit(vocabulary)\n",
    "x_comment_text = np.array(list(vocabulary_processor.fit_transform(comment_text_list_clean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 4. make test train\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "train_proportion = 0.8\n",
    "shuffled_idx = np.random.permutation(np.arange(len(x_comment_text)))\n",
    "\n",
    "train_idx = np.random.choice(shuffled_idx, size = int(train_proportion * len(shuffled_idx)), replace = False)\n",
    "development_idx = np.array([x for x in shuffled_idx if not x in train_idx])\n",
    "\n",
    "x_comment_text_train = x_comment_text[train_idx]\n",
    "\n",
    "toxic_labels_train = toxic_labels[train_idx]\n",
    "severe_toxic_labels_train = severe_toxic_labels[train_idx]\n",
    "obscene_labels_train = obscene_labels[train_idx]\n",
    "threat_labels_train = threat_labels[train_idx]\n",
    "insult_labels_train = insult_labels[train_idx]\n",
    "identity_hate_labels_train = identity_hate_labels[train_idx]\n",
    "\n",
    "x_comment_text_development = x_comment_text[development_idx]\n",
    "\n",
    "toxic_labels_development = toxic_labels[development_idx]\n",
    "severe_toxic_labels_development = severe_toxic_labels[development_idx]\n",
    "obscene_labels_development = obscene_labels[development_idx]\n",
    "threat_labels_development = threat_labels[development_idx]\n",
    "insult_labels_development = insult_labels[development_idx]\n",
    "identity_hate_labels_development = identity_hate_labels[development_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## constants\n",
    "\n",
    "SEQUENCE_LENGTH = MAX_DOCUMENT_LENGTH\n",
    "NUM_CLASSES = toxic_labels_train.shape[1]\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## 6. make placeholders\n",
    "\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    comment_text_placeholder = tf.placeholder(tf.int32, shape = (None, SEQUENCE_LENGTH))\n",
    "\n",
    "    toxic_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    severe_toxic_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    obscene_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    threat_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    insult_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "    identity_hate_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## train parameters\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 100\n",
    "NUM_DISPLAY_STEPS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "#VOCABULARY_SIZE = len(vocabulary_processor.vocabulary_)\n",
    "#EMBEDDING_SIZE = 50\n",
    "NUM_FILTERS = 10\n",
    "KERNEL_SIZE = 1\n",
    "FILTER_SHAPE = [KERNEL_SIZE, EMBEDDING_SIZE, NUM_FILTERS]\n",
    "POOLING_SIZE = SEQUENCE_LENGTH - KERNEL_SIZE + 1\n",
    "STRIDE = 1\n",
    "NUM_HIDDEN_1 = int(NUM_FILTERS/2)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## 7. define network\n",
    "\n",
    "def make_hidden_1(inputs, name):\n",
    "     return tf.layers.dense(\n",
    "        inputs = inputs, \n",
    "        units = NUM_UNITS_HIDDEN_1,\n",
    "        name = name\n",
    "     )\n",
    "    \n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.variable_scope(\"network\", reuse = tf.AUTO_REUSE):\n",
    "\n",
    "        W = tf.Variable(\n",
    "            initial_value = tf.constant(0.0, shape = [VOCABULARY_SIZE, EMBEDDING_SIZE]), # why constant?\n",
    "            trainable = False, # try True for non-static\n",
    "            name = \"W\"\n",
    "        )\n",
    "        embedding_placeholder = tf.placeholder(tf.float32, [VOCABULARY_SIZE, EMBEDDING_SIZE])\n",
    "        embedding_init = W.assign(embedding_placeholder)\n",
    "\n",
    "        # shape: [batch, SEQUENCE_LENGTH, EMBEDDING_SIZE]\n",
    "\n",
    "        filter_1_coefficients = tf.Variable(\n",
    "            initial_value = tf.truncated_normal(FILTER_SHAPE, stddev = 0.1), \n",
    "            name = \"filter_1_coefficients\"\n",
    "        )\n",
    "        filter_1 = tf.nn.conv1d(\n",
    "            value = word_embeddings,\n",
    "            filters = filter_1_coefficients,\n",
    "            stride = STRIDE,\n",
    "            padding = \"VALID\",\n",
    "            name = \"filter_1\"\n",
    "        )\n",
    "\n",
    "        filter_1_bias = tf.Variable(tf.constant(0.1, shape = [NUM_FILTERS]), name = \"filter_1_bias\")\n",
    "        # initialiser?\n",
    "        convolution_1 = tf.nn.relu(tf.nn.bias_add(filter_1, filter_1_bias), name = \"convolution_1\")\n",
    "        \n",
    "        # print(convolution_1.shape) # [batch, SEQUENCE_LENGTH - KERNEL_SIZE + 1, NUM_FILTERS]\n",
    "        \n",
    "        max_pooling1d = tf.nn.pool(\n",
    "            input = convolution_1,\n",
    "            window_shape = [POOLING_SIZE],\n",
    "            pooling_type = \"MAX\",\n",
    "            strides = [1],\n",
    "            padding = 'VALID',\n",
    "            name = \"max_pooling1d\"\n",
    "        )\n",
    "        # print(max_pooling1d.shape) # [batch, 1, NUM_FILTERS]\n",
    "        \n",
    "        max_pooling1d_flattened = tf.contrib.layers.flatten(max_pooling1d)\n",
    "        # print(max_pooling1d_flattened.shape) # [batch, 1, NUM_FILTERS]\n",
    "        \n",
    "        #toxic_dense = tf.layers.dense(\n",
    "        #    inputs = max_pooling1d_flattened, \n",
    "        #    units = NUM_HIDDEN_1,\n",
    "        #    activation = tf.nn.relu\n",
    "        #)\n",
    "        \n",
    "        #toxic_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"toxic_dense\")\n",
    "        #severe_toxic_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"severe_toxic_dense\")\n",
    "        #obscene_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"obscene_dense\")\n",
    "        #threat_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"threat_dense\")\n",
    "        #insult_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"insult_dense\")\n",
    "        #identity_hate_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"identity_hate_dense\")\n",
    "\n",
    "        # initialiser?\n",
    "        toxic_logits =  tf.layers.dense(\n",
    "            inputs = max_pooling1d_flattened, \n",
    "            units = NUM_CLASSES,\n",
    "            name = \"toxic_logits\"\n",
    "        )\n",
    "        #severe_toxic_logits = make_logits(inputs = toxic_dense, name = \"severe_toxic_logits\")\n",
    "        #obscene_logits = make_logits(inputs = toxic_dense, name = \"obscene_logits\")\n",
    "        #threat_logits = make_logits(inputs = toxic_dense, name = \"threat_logits\")\n",
    "        #insult_logits = make_logits(inputs = toxic_dense, name = \"insult_logits\")\n",
    "        #identity_hate_logits = make_logits(inputs = toxic_dense, name = \"identity_hate_logits\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 8. define loss\n",
    "\n",
    "def make_single_loss(logits, placeholder, name):\n",
    "    sigmoid_cross_entropy_op = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = logits, \n",
    "            labels = placeholder\n",
    "        )\n",
    "    return tf.reduce_mean(sigmoid_cross_entropy_op, name = name)\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "\n",
    "        toxic_loss_op = make_single_loss(toxic_logits, toxic_placeholder, \"toxic_loss_op\")\n",
    "        #severe_toxic_loss_op = make_single_loss(severe_toxic_logits, severe_toxic_placeholder, \"severe_toxic_loss_op\")\n",
    "        #obscene_loss_op = make_single_loss(obscene_logits, obscene_placeholder, \"obscene_loss_op\")\n",
    "        #threat_loss_op = make_single_loss(threat_logits, threat_placeholder, \"threat_loss_op\")\n",
    "        #insult_loss_op = make_single_loss(insult_logits, insult_placeholder, \"insult_loss_op\")\n",
    "        #identity_hate_loss_op = make_single_loss(identity_hate_logits, identity_hate_placeholder, \"identity_hate_loss_op\")\n",
    "\n",
    "        loss_op = tf.reduce_mean([toxic_loss_op])\n",
    "                                  #, severe_toxic_loss_op, obscene_loss_op, \n",
    "        #                          threat_loss_op, insult_loss_op, identity_hate_loss_op])\n",
    "\n",
    "        toxic_loss_summary_op = tf.summary.scalar(\"toxic_loss_op\", toxic_loss_op)\n",
    "        #severe_toxic_loss_op_summary = tf.summary.scalar(\"severe_toxic_loss_op\", severe_toxic_loss_op)\n",
    "        #obscene_loss_op_summary = tf.summary.scalar(\"obscene_loss_op\", obscene_loss_op)\n",
    "        #threat_loss_op_summary = tf.summary.scalar(\"threat_loss_op\", threat_loss_op)\n",
    "        #insult_loss_op_summary = tf.summary.scalar(\"insult_loss_op\", insult_loss_op)\n",
    "        #identity_hate_loss_op_summary = tf.summary.scalar(\"identity_hate_loss_op\", identity_hate_loss_op)\n",
    "\n",
    "        loss_summary_op = tf.summary.scalar(\"loss_op\", loss_op)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name network/embedding_coefficients:0/grad/hist is illegal; using network/embedding_coefficients_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/embedding_coefficients:0/grad/sparsity is illegal; using network/embedding_coefficients_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name network/filter_1_coefficients:0/grad/hist is illegal; using network/filter_1_coefficients_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/filter_1_coefficients:0/grad/sparsity is illegal; using network/filter_1_coefficients_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name network/filter_1_bias:0/grad/hist is illegal; using network/filter_1_bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/filter_1_bias:0/grad/sparsity is illegal; using network/filter_1_bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name network/toxic_logits/kernel:0/grad/hist is illegal; using network/toxic_logits/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/toxic_logits/kernel:0/grad/sparsity is illegal; using network/toxic_logits/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name network/toxic_logits/bias:0/grad/hist is illegal; using network/toxic_logits/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name network/toxic_logits/bias:0/grad/sparsity is illegal; using network/toxic_logits/bias_0/grad/sparsity instead.\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "import time, os\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## 9. define train_op\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.variable_scope(\"train\", reuse = tf.AUTO_REUSE):\n",
    "\n",
    "        global_step = tf.Variable(0, name = \"global_step\", trainable = False)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE)\n",
    "        grads_and_vars = optimizer.compute_gradients(loss_op)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step)\n",
    "        \n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([toxic_loss_summary_op, loss_summary_op, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(LOG_DIRECTORY, \"summaries\", \"train\")\n",
    "        #train_summary_writer = tf.summary.FileWriter(train_summary_dir, tf.get_default_graph())\n",
    "\n",
    "        # Dev summaries\n",
    "        development_summary_op = tf.summary.merge([toxic_loss_summary_op, loss_summary_op])\n",
    "        development_summary_dir = os.path.join(LOG_DIRECTORY, \"summaries\", \"development\")\n",
    "        #development_summary_writer = tf.summary.FileWriter(development_summary_dir, tf.get_default_graph())\n",
    "    \n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.333555, development_loss 0.306562, current_step 100, took 4.00971\n",
      "train_loss 0.269191, development_loss 0.296797, current_step 200, took 49.0388\n",
      "train_loss 0.294905, development_loss 0.288521, current_step 300, took 14.1569\n",
      "train_loss 0.34962, development_loss 0.279104, current_step 400, took 12.2418\n",
      "train_loss 0.377331, development_loss 0.265336, current_step 500, took 11.4824\n",
      "train_loss 0.195575, development_loss 0.248661, current_step 600, took 8.6897\n",
      "train_loss 0.120533, development_loss 0.229447, current_step 700, took 8.65263\n",
      "train_loss 0.222935, development_loss 0.212975, current_step 800, took 9.19015\n",
      "train_loss 0.242483, development_loss 0.198044, current_step 900, took 7.69971\n",
      "train_loss 0.152757, development_loss 0.185029, current_step 1000, took 8.98394\n",
      "train_loss 0.0855937, development_loss 0.176606, current_step 1100, took 8.11932\n",
      "train_loss 0.201223, development_loss 0.169881, current_step 1200, took 7.86171\n",
      "train_loss 0.0567818, development_loss 0.164634, current_step 1300, took 7.78546\n",
      "train_loss 0.123148, development_loss 0.16207, current_step 1400, took 9.772\n",
      "train_loss 0.265444, development_loss 0.157689, current_step 1500, took 10.6105\n",
      "train_loss 0.135432, development_loss 0.153639, current_step 1600, took 8.32237\n",
      "train_loss 0.109525, development_loss 0.151111, current_step 1700, took 7.26464\n",
      "train_loss 0.183809, development_loss 0.148468, current_step 1800, took 7.95299\n",
      "train_loss 0.157222, development_loss 0.145244, current_step 1900, took 7.69548\n",
      "train_loss 0.0779516, development_loss 0.143985, current_step 2000, took 7.65914\n",
      "train_loss 0.0482665, development_loss 0.144027, current_step 2100, took 7.92089\n",
      "train_loss 0.188474, development_loss 0.141488, current_step 2200, took 8.22003\n",
      "train_loss 0.0905589, development_loss 0.141367, current_step 2300, took 7.57922\n",
      "train_loss 0.160584, development_loss 0.139382, current_step 2400, took 8.86107\n",
      "train_loss 0.143151, development_loss 0.138437, current_step 2500, took 8.58909\n",
      "train_loss 0.370954, development_loss 0.137619, current_step 2600, took 8.35007\n",
      "train_loss 0.215638, development_loss 0.137164, current_step 2700, took 8.3871\n",
      "train_loss 0.140136, development_loss 0.137457, current_step 2800, took 8.03197\n",
      "train_loss 0.0903456, development_loss 0.135638, current_step 2900, took 7.01485\n",
      "train_loss 0.254868, development_loss 0.135112, current_step 3000, took 6.78517\n",
      "train_loss 0.165145, development_loss 0.13453, current_step 3100, took 7.03654\n",
      "train_loss 0.166392, development_loss 0.134953, current_step 3200, took 7.5021\n",
      "train_loss 0.119864, development_loss 0.133791, current_step 3300, took 7.25729\n",
      "train_loss 0.169742, development_loss 0.133253, current_step 3400, took 6.9855\n",
      "train_loss 0.219043, development_loss 0.133796, current_step 3500, took 6.99127\n",
      "train_loss 0.104265, development_loss 0.132838, current_step 3600, took 7.97725\n",
      "train_loss 0.107707, development_loss 0.132115, current_step 3700, took 7.00529\n",
      "train_loss 0.0809693, development_loss 0.13149, current_step 3800, took 6.96283\n",
      "train_loss 0.0565034, development_loss 0.131438, current_step 3900, took 7.31928\n",
      "train_loss 0.0579047, development_loss 0.131688, current_step 4000, took 7.10813\n",
      "train_loss 0.107034, development_loss 0.130523, current_step 4100, took 7.77922\n",
      "train_loss 0.102106, development_loss 0.131025, current_step 4200, took 7.34532\n",
      "train_loss 0.103482, development_loss 0.129445, current_step 4300, took 7.33096\n",
      "train_loss 0.0305441, development_loss 0.131011, current_step 4400, took 6.83895\n",
      "train_loss 0.0969121, development_loss 0.128649, current_step 4500, took 7.43127\n",
      "train_loss 0.0629653, development_loss 0.128419, current_step 4600, took 7.25866\n",
      "train_loss 0.297664, development_loss 0.129469, current_step 4700, took 6.83113\n",
      "train_loss 0.374255, development_loss 0.129902, current_step 4800, took 7.13125\n",
      "train_loss 0.0861074, development_loss 0.128064, current_step 4900, took 7.11811\n",
      "train_loss 0.113939, development_loss 0.127566, current_step 5000, took 6.9046\n",
      "train_loss 0.182765, development_loss 0.126804, current_step 5100, took 6.8523\n",
      "train_loss 0.110829, development_loss 0.126625, current_step 5200, took 7.07537\n",
      "train_loss 0.309484, development_loss 0.126472, current_step 5300, took 7.5202\n",
      "train_loss 0.252986, development_loss 0.125881, current_step 5400, took 7.36314\n",
      "train_loss 0.181774, development_loss 0.126215, current_step 5500, took 7.45716\n",
      "train_loss 0.177621, development_loss 0.125849, current_step 5600, took 7.92726\n",
      "train_loss 0.115486, development_loss 0.125819, current_step 5700, took 7.0203\n",
      "train_loss 0.110514, development_loss 0.126495, current_step 5800, took 7.63965\n",
      "train_loss 0.125952, development_loss 0.126306, current_step 5900, took 8.98454\n",
      "train_loss 0.100168, development_loss 0.126666, current_step 6000, took 10.3771\n",
      "train_loss 0.0807801, development_loss 0.126011, current_step 6100, took 8.33105\n",
      "train_loss 0.0384657, development_loss 0.126032, current_step 6200, took 8.31464\n",
      "train_loss 0.0449154, development_loss 0.126107, current_step 6300, took 6.94014\n",
      "train_loss 0.0626022, development_loss 0.125268, current_step 6400, took 7.29724\n",
      "train_loss 0.0347162, development_loss 0.125942, current_step 6500, took 7.66352\n",
      "train_loss 0.1952, development_loss 0.125691, current_step 6600, took 7.71322\n",
      "train_loss 0.0479424, development_loss 0.125658, current_step 6700, took 7.89698\n",
      "train_loss 0.137437, development_loss 0.127141, current_step 6800, took 8.16817\n",
      "train_loss 0.0227272, development_loss 0.124089, current_step 6900, took 7.08602\n",
      "train_loss 0.103704, development_loss 0.124369, current_step 7000, took 6.89601\n",
      "train_loss 0.148094, development_loss 0.124161, current_step 7100, took 7.38834\n",
      "train_loss 0.0429721, development_loss 0.124624, current_step 7200, took 8.27707\n",
      "train_loss 0.120029, development_loss 0.123889, current_step 7300, took 9.1335\n",
      "train_loss 0.0881132, development_loss 0.12408, current_step 7400, took 7.06443\n",
      "train_loss 0.0969832, development_loss 0.124069, current_step 7500, took 8.05995\n",
      "train_loss 0.0399017, development_loss 0.123696, current_step 7600, took 7.38436\n",
      "train_loss 0.168313, development_loss 0.123185, current_step 7700, took 7.0271\n",
      "train_loss 0.0642231, development_loss 0.124462, current_step 7800, took 7.54099\n",
      "train_loss 0.109286, development_loss 0.123002, current_step 7900, took 7.32023\n",
      "train_loss 0.0751378, development_loss 0.123927, current_step 8000, took 7.95524\n",
      "train_loss 0.0874533, development_loss 0.122782, current_step 8100, took 7.3069\n",
      "train_loss 0.104637, development_loss 0.125054, current_step 8200, took 7.13649\n",
      "train_loss 0.0718631, development_loss 0.124157, current_step 8300, took 7.20269\n",
      "train_loss 0.0552962, development_loss 0.12343, current_step 8400, took 7.94344\n",
      "train_loss 0.0585225, development_loss 0.123136, current_step 8500, took 7.7222\n",
      "train_loss 0.143088, development_loss 0.123765, current_step 8600, took 9.31912\n",
      "train_loss 0.0970855, development_loss 0.12425, current_step 8700, took 7.88544\n",
      "train_loss 0.306238, development_loss 0.125106, current_step 8800, took 6.8694\n",
      "train_loss 0.0555917, development_loss 0.123638, current_step 8900, took 6.78043\n",
      "train_loss 0.0492807, development_loss 0.123532, current_step 9000, took 7.19087\n",
      "train_loss 0.314398, development_loss 0.124407, current_step 9100, took 7.64135\n",
      "train_loss 0.0991897, development_loss 0.123257, current_step 9200, took 8.51364\n",
      "train_loss 0.0966294, development_loss 0.123299, current_step 9300, took 7.45493\n",
      "train_loss 0.0389805, development_loss 0.123147, current_step 9400, took 7.18741\n",
      "train_loss 0.0360502, development_loss 0.124461, current_step 9500, took 7.12848\n",
      "train_loss 0.126047, development_loss 0.122662, current_step 9600, took 6.89998\n",
      "train_loss 0.0511646, development_loss 0.12318, current_step 9700, took 7.14459\n",
      "train_loss 0.128741, development_loss 0.125792, current_step 9800, took 7.03207\n",
      "train_loss 0.017861, development_loss 0.124554, current_step 9900, took 7.55036\n",
      "train_loss 0.0265306, development_loss 0.123745, current_step 10000, took 8.95006\n",
      "train_loss 0.127336, development_loss 0.123259, current_step 10100, took 9.16245\n",
      "train_loss 0.0827645, development_loss 0.122766, current_step 10200, took 7.11281\n",
      "train_loss 0.124044, development_loss 0.122619, current_step 10300, took 7.99933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.0390506, development_loss 0.122527, current_step 10400, took 6.85071\n",
      "train_loss 0.083262, development_loss 0.122244, current_step 10500, took 7.32902\n",
      "train_loss 0.233549, development_loss 0.122661, current_step 10600, took 7.3616\n",
      "train_loss 0.0609427, development_loss 0.122744, current_step 10700, took 7.33846\n",
      "train_loss 0.0723832, development_loss 0.123439, current_step 10800, took 9.63326\n",
      "train_loss 0.0886111, development_loss 0.125043, current_step 10900, took 9.10025\n",
      "train_loss 0.0562639, development_loss 0.123709, current_step 11000, took 6.93142\n",
      "train_loss 0.059766, development_loss 0.124836, current_step 11100, took 8.07963\n",
      "train_loss 0.169509, development_loss 0.123941, current_step 11200, took 7.61744\n",
      "train_loss 0.0857117, development_loss 0.124443, current_step 11300, took 7.20486\n",
      "train_loss 0.0880783, development_loss 0.124571, current_step 11400, took 8.33859\n",
      "train_loss 0.0284383, development_loss 0.123836, current_step 11500, took 9.27781\n",
      "train_loss 0.0220443, development_loss 0.123362, current_step 11600, took 8.17558\n",
      "train_loss 0.0408083, development_loss 0.123017, current_step 11700, took 8.15646\n",
      "train_loss 0.0658133, development_loss 0.124037, current_step 11800, took 7.20261\n",
      "train_loss 0.187807, development_loss 0.126424, current_step 11900, took 7.19754\n",
      "train_loss 0.0327366, development_loss 0.122039, current_step 12000, took 8.85596\n",
      "train_loss 0.0414141, development_loss 0.12267, current_step 12100, took 8.38308\n",
      "train_loss 0.164039, development_loss 0.122426, current_step 12200, took 11.0382\n",
      "train_loss 0.0552018, development_loss 0.122779, current_step 12300, took 9.04754\n",
      "train_loss 0.131544, development_loss 0.122139, current_step 12400, took 8.09171\n",
      "train_loss 0.0723623, development_loss 0.122754, current_step 12500, took 7.54516\n",
      "train_loss 0.0880059, development_loss 0.122538, current_step 12600, took 8.41425\n",
      "train_loss 0.0494449, development_loss 0.122404, current_step 12700, took 8.22779\n",
      "train_loss 0.0518263, development_loss 0.122594, current_step 12800, took 8.82602\n",
      "train_loss 0.0687841, development_loss 0.123714, current_step 12900, took 8.63991\n",
      "train_loss 0.0250823, development_loss 0.121641, current_step 13000, took 7.63711\n",
      "train_loss 0.0909427, development_loss 0.121966, current_step 13100, took 7.24266\n",
      "train_loss 0.0620306, development_loss 0.122666, current_step 13200, took 6.86865\n",
      "train_loss 0.0910371, development_loss 0.126672, current_step 13300, took 6.6582\n",
      "train_loss 0.038285, development_loss 0.123167, current_step 13400, took 6.76625\n",
      "train_loss 0.0312622, development_loss 0.123336, current_step 13500, took 7.38638\n",
      "train_loss 0.116345, development_loss 0.122938, current_step 13600, took 7.32416\n",
      "train_loss 0.0633781, development_loss 0.12311, current_step 13700, took 6.9851\n",
      "train_loss 0.0322158, development_loss 0.123754, current_step 13800, took 6.86747\n",
      "train_loss 0.0366008, development_loss 0.12607, current_step 13900, took 7.30031\n",
      "train_loss 0.0631954, development_loss 0.123599, current_step 14000, took 6.91552\n",
      "train_loss 0.0664931, development_loss 0.124154, current_step 14100, took 8.39271\n",
      "train_loss 0.0777672, development_loss 0.123479, current_step 14200, took 7.94622\n",
      "train_loss 0.152744, development_loss 0.123968, current_step 14300, took 7.27069\n",
      "train_loss 0.108157, development_loss 0.122024, current_step 14400, took 8.48438\n",
      "train_loss 0.107784, development_loss 0.122169, current_step 14500, took 8.10484\n",
      "train_loss 0.131566, development_loss 0.124195, current_step 14600, took 6.89234\n",
      "train_loss 0.0603262, development_loss 0.122744, current_step 14700, took 7.16841\n",
      "train_loss 0.136673, development_loss 0.122929, current_step 14800, took 7.45096\n",
      "train_loss 0.0209073, development_loss 0.125225, current_step 14900, took 7.67501\n",
      "train_loss 0.0561102, development_loss 0.123469, current_step 15000, took 9.16806\n",
      "train_loss 0.13688, development_loss 0.125355, current_step 15100, took 7.13805\n",
      "train_loss 0.0277744, development_loss 0.122869, current_step 15200, took 8.6609\n",
      "train_loss 0.0576338, development_loss 0.123359, current_step 15300, took 9.82765\n",
      "train_loss 0.0621782, development_loss 0.122659, current_step 15400, took 8.00707\n",
      "train_loss 0.0397915, development_loss 0.122839, current_step 15500, took 8.35665\n",
      "train_loss 0.0792113, development_loss 0.12299, current_step 15600, took 7.81672\n",
      "train_loss 0.0727896, development_loss 0.12271, current_step 15700, took 7.09222\n",
      "train_loss 0.0310029, development_loss 0.123, current_step 15800, took 6.92671\n",
      "train_loss 0.0782266, development_loss 0.123422, current_step 15900, took 7.42141\n",
      "train_loss 0.0414348, development_loss 0.125555, current_step 16000, took 9.26276\n",
      "train_loss 0.0613716, development_loss 0.125186, current_step 16100, took 8.34036\n",
      "train_loss 0.126796, development_loss 0.124645, current_step 16200, took 6.96035\n",
      "train_loss 0.0620406, development_loss 0.123928, current_step 16300, took 6.6665\n",
      "train_loss 0.0590228, development_loss 0.124731, current_step 16400, took 6.71295\n",
      "train_loss 0.0328518, development_loss 0.125344, current_step 16500, took 6.99523\n",
      "train_loss 0.0685363, development_loss 0.125684, current_step 16600, took 7.73206\n",
      "train_loss 0.0114865, development_loss 0.124527, current_step 16700, took 7.78766\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## session\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        sess.run(embedding_init, feed_dict = {embedding_placeholder: embedding})\n",
    "        \n",
    "        start_of_step_time = default_timer()\n",
    "        \n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "            idx_batches = make_idx_batches(x_comment_text_train, BATCH_SIZE)\n",
    "            \n",
    "            development_loss_last = 100\n",
    "\n",
    "            for idx_batch in idx_batches:\n",
    "\n",
    "                x_comment_text_batch = x_comment_text_train[idx_batch]\n",
    "\n",
    "                toxic_labels_batch = toxic_labels_train[idx_batch]\n",
    "                #severe_toxic_labels_batch = severe_toxic_labels_train[idx_batch]\n",
    "                #obscene_labels_batch = obscene_labels_train[idx_batch]\n",
    "                #threat_labels_batch = threat_labels_train[idx_batch]\n",
    "                #insult_labels_batch = insult_labels_train[idx_batch]\n",
    "                #identity_hate_labels_batch = identity_hate_labels_train[idx_batch]\n",
    "\n",
    "                # train\n",
    "                train_feed_dict = {\n",
    "                    comment_text_placeholder: x_comment_text_batch,\n",
    "                    toxic_placeholder: toxic_labels_batch,\n",
    "                 #   severe_toxic_placeholder: severe_toxic_labels_batch, \n",
    "                 #   obscene_placeholder: obscene_labels_batch,\n",
    "                 #   threat_placeholder: threat_labels_batch,\n",
    "                 #   insult_placeholder: insult_labels_batch,\n",
    "                 #   identity_hate_placeholder: identity_hate_labels_batch\n",
    "                }\n",
    "\n",
    "                _, train_loss, train_summary = session.run([train_op, loss_op, train_summary_op], feed_dict = train_feed_dict)\n",
    "\n",
    "                current_step = tf.train.global_step(session, global_step)\n",
    "                \n",
    "                #train_summary_writer.add_summary(train_summary, current_step)\n",
    "\n",
    "                if current_step % NUM_DISPLAY_STEPS == 0:\n",
    "                    \n",
    "                    took = default_timer() - start_of_step_time\n",
    "                    start_of_step_time = default_timer()\n",
    "\n",
    "                    development_feed_dict = {\n",
    "                        comment_text_placeholder: x_comment_text_development,\n",
    "                        toxic_placeholder: toxic_labels_development,\n",
    "                    #    severe_toxic_placeholder: severe_toxic_labels_development, \n",
    "                    #    obscene_placeholder: obscene_labels_development,\n",
    "                    #    threat_placeholder: threat_labels_development,\n",
    "                    #    insult_placeholder: insult_labels_development,\n",
    "                    #    identity_hate_placeholder: identity_hate_labels_development\n",
    "                    }\n",
    "\n",
    "                    development_loss, development_summary = session.run(\n",
    "                        fetches = [loss_op, development_summary_op], \n",
    "                        feed_dict = development_feed_dict\n",
    "                    )\n",
    "                    \n",
    "                    print(\"train_loss {:g}, development_loss {:g}, current_step {:g}, took {:g}\".format(\n",
    "                        train_loss, development_loss, current_step, took))\n",
    "                        \n",
    "                    if development_loss - development_loss_last > 0:\n",
    "                        if patience_counter > 10: \n",
    "                            break\n",
    "                        patience_counter = patience_counter + 1\n",
    "                    else:\n",
    "                        patience_counter = 0\n",
    "                        \n",
    "                    development_loss_last = development_loss\n",
    "                    \n",
    "                    #development_summary_writer.add_summary(development_summary, current_step)\n",
    "                    \n",
    "            if development_loss - development_loss_last > 0: \n",
    "                break\n",
    "                    \n",
    "                    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
