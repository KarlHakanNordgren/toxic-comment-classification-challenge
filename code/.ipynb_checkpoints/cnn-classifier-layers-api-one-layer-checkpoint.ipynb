{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "# https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## plan\n",
    "\n",
    "# 1. open dataset\n",
    "# 2. clean dataset\n",
    "# 3. munge dataset\n",
    "# 4. make test train\n",
    "# 5. make constants\n",
    "# 6. make placeholders\n",
    "# 7. define network\n",
    "# 8. define loss\n",
    "# 9. define train_op\n",
    "# 10. define development_op\n",
    "# 11. seesion: epochs, batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "######################################################\n",
    "## 5. make constants\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "ROOT_LOG_DIRECTORY = \"../output/tf-logs\"\n",
    "LOG_DIRECTORY = \"{}/run-{}/\".format(ROOT_LOG_DIRECTORY, now)\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "def make_idx_batches(data, batch_size):\n",
    "    idx_array = np.arange(data.shape[0])\n",
    "    n = len(idx_array)\n",
    "    res = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        res.append(idx_array[i:(i+batch_size)])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 1. open dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "response_df = pd.read_pickle(\"../output/response_df.pkl\")\n",
    "train_df = pd.read_pickle(\"../output/train_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 2. clean dataset\n",
    "## 3. munge dataset\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "def clean_string(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'m\", \" \\'m\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "comment_text_list = train_df['comment_text'].str.lower().tolist()\n",
    "comment_text_list_clean = [clean_string(x) for x in comment_text_list]\n",
    "\n",
    "toxic_list = train_df['toxic'].tolist()\n",
    "toxic_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in toxic_list])\n",
    "\n",
    "severe_toxic_list = train_df['severe_toxic'].tolist()\n",
    "severe_toxic_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in severe_toxic_list])\n",
    "\n",
    "obscene_list = train_df['obscene'].tolist()\n",
    "obscene_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in obscene_list])\n",
    "\n",
    "threat_list = train_df['threat'].tolist()\n",
    "threat_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in threat_list])\n",
    "\n",
    "insult_list = train_df['insult'].tolist()\n",
    "insult_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in insult_list])\n",
    "\n",
    "identity_hate_list = train_df['identity_hate'].tolist()\n",
    "identity_hate_labels = np.array([[0, 1] if x == 1 else [1, 0] for x in identity_hate_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 2000\n",
    "MIN_FREQUENCY = 10\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "from tensorflow.contrib.learn import preprocessing\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "max_document_length = max([len(x.split(\" \")) for x in comment_text_list_clean])\n",
    "vocabulary_processor = preprocessing.VocabularyProcessor(\n",
    "    max_document_length = MAX_DOCUMENT_LENGTH,\n",
    "    min_frequency = MIN_FREQUENCY\n",
    ")\n",
    "\n",
    "x_comment_text = np.array(list(vocabulary_processor.fit_transform(comment_text_list_clean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 4. make test train\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "train_proportion = 0.8\n",
    "shuffled_idx = np.random.permutation(np.arange(len(x_comment_text)))\n",
    "\n",
    "train_idx = np.random.choice(shuffled_idx, size = int(train_proportion * len(shuffled_idx)), replace = False)\n",
    "development_idx = np.array([x for x in shuffled_idx if not x in train_idx])\n",
    "\n",
    "x_comment_text_train = x_comment_text[train_idx]\n",
    "\n",
    "toxic_labels_train = toxic_labels[train_idx]\n",
    "severe_toxic_labels_train = severe_toxic_labels[train_idx]\n",
    "obscene_labels_train = obscene_labels[train_idx]\n",
    "threat_labels_train = threat_labels[train_idx]\n",
    "insult_labels_train = insult_labels[train_idx]\n",
    "identity_hate_labels_train = identity_hate_labels[train_idx]\n",
    "\n",
    "x_comment_text_development = x_comment_text[development_idx]\n",
    "\n",
    "toxic_labels_development = toxic_labels[development_idx]\n",
    "severe_toxic_labels_development = severe_toxic_labels[development_idx]\n",
    "obscene_labels_development = obscene_labels[development_idx]\n",
    "threat_labels_development = threat_labels[development_idx]\n",
    "insult_labels_development = insult_labels[development_idx]\n",
    "identity_hate_labels_development = identity_hate_labels[development_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## constants\n",
    "\n",
    "SEQUENCE_LENGTH = MAX_DOCUMENT_LENGTH\n",
    "NUM_CLASSES = toxic_labels_train.shape[1]\n",
    "\n",
    "VOCABULARY_SIZE = len(vocabulary_processor.vocabulary_)\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "NUM_FILTERS = 8\n",
    "KERNEL_SIZE = 3\n",
    "FILTER_SHAPE = [KERNEL_SIZE, EMBEDDING_SIZE, NUM_FILTERS]\n",
    "\n",
    "STRIDE = 1\n",
    "\n",
    "POOLING_SIZE = SEQUENCE_LENGTH - KERNEL_SIZE + 1\n",
    "POOLING_WINDOW_SHAPE = [1, POOLING_SIZE, 1]\n",
    "\n",
    "NUM_UNITS_HIDDEN_1 = 8\n",
    "NUM_CLASSES = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## 6. make placeholders\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "comment_text_placeholder = tf.placeholder(tf.int32, shape = (None, SEQUENCE_LENGTH))\n",
    "\n",
    "toxic_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "severe_toxic_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "obscene_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "threat_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "insult_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "identity_hate_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, NUM_CLASSES))\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## 7. define network\n",
    "\n",
    "def make_hidden_1(inputs, name):\n",
    "     return tf.layers.dense(\n",
    "        inputs = inputs, \n",
    "        units = NUM_UNITS_HIDDEN_1,\n",
    "        name = name\n",
    "     )\n",
    "    \n",
    "def make_logits(inputs, name):\n",
    "    return tf.layers.dense(\n",
    "        inputs = inputs, \n",
    "        units = NUM_CLASSES,\n",
    "        name = name\n",
    "     )\n",
    "\n",
    "with tf.name_scope(\"network\"):\n",
    "        \n",
    "    embedding_coefficients = tf.Variable(\n",
    "        tf.random_uniform([VOCABULARY_SIZE, EMBEDDING_SIZE], -1.0, 1.0), \n",
    "        name = \"embedding_coefficients\"\n",
    "    )\n",
    "    word_embeddings = tf.nn.embedding_lookup(embedding_coefficients, comment_text_placeholder)\n",
    "    # shape: [batch, SEQUENCE_LENGTH, EMBEDDING_SIZE]\n",
    "    \n",
    "    conv1d = tf.layers.conv1d(\n",
    "        inputs = word_embeddings, \n",
    "        filters = NUM_FILTERS,\n",
    "        kernel_size = KERNEL_SIZE,\n",
    "        strides = 1,\n",
    "        padding = 'valid',\n",
    "        activation = tf.nn.relu\n",
    "    )\n",
    "\n",
    "    max_pooling1d = tf.layers.max_pooling1d(\n",
    "        inputs = conv1d, \n",
    "        pool_size = POOLING_SIZE,\n",
    "        strides = 1,\n",
    "        padding = 'valid',\n",
    "    )\n",
    "\n",
    "    # Flatten the data to a 1-D vector for the fully connected layer\n",
    "    max_pooling1d_flattened = tf.contrib.layers.flatten(max_pooling1d)\n",
    "\n",
    "    toxic_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"toxic_dense\")\n",
    "    #severe_toxic_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"severe_toxic_dense\")\n",
    "    #obscene_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"obscene_dense\")\n",
    "    #threat_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"threat_dense\")\n",
    "    #insult_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"insult_dense\")\n",
    "    #identity_hate_dense = make_hidden_1(inputs = max_pooling1d_flattened, name = \"identity_hate_dense\")\n",
    "    \n",
    "    toxic_logits = make_logits(inputs = toxic_dense, name = \"toxic_logits\")\n",
    "    #severe_toxic_logits = make_logits(inputs = toxic_dense, name = \"severe_toxic_logits\")\n",
    "    #obscene_logits = make_logits(inputs = toxic_dense, name = \"obscene_logits\")\n",
    "    #threat_logits = make_logits(inputs = toxic_dense, name = \"threat_logits\")\n",
    "    #insult_logits = make_logits(inputs = toxic_dense, name = \"insult_logits\")\n",
    "    #identity_hate_logits = make_logits(inputs = toxic_dense, name = \"identity_hate_logits\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 8. define loss\n",
    "\n",
    "def make_single_loss(logits, placeholder, name):\n",
    "    sigmoid_cross_entropy_op = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = logits, \n",
    "            labels = placeholder\n",
    "        )\n",
    "    return tf.reduce_mean(sigmoid_cross_entropy_op, name = name)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    \n",
    "    toxic_loss_op = make_single_loss(toxic_logits, toxic_placeholder, \"toxic_loss_op\")\n",
    "    #severe_toxic_loss_op = make_single_loss(severe_toxic_logits, severe_toxic_placeholder, \"severe_toxic_loss_op\")\n",
    "    #obscene_loss_op = make_single_loss(obscene_logits, obscene_placeholder, \"obscene_loss_op\")\n",
    "    #threat_loss_op = make_single_loss(threat_logits, threat_placeholder, \"threat_loss_op\")\n",
    "    #insult_loss_op = make_single_loss(insult_logits, insult_placeholder, \"insult_loss_op\")\n",
    "    #identity_hate_loss_op = make_single_loss(identity_hate_logits, identity_hate_placeholder, \"identity_hate_loss_op\")\n",
    "    \n",
    "    #loss_op = tf.reduce_mean([toxic_loss_op, severe_toxic_loss_op, obscene_loss_op, \n",
    "    #                          threat_loss_op, insult_loss_op, identity_hate_loss_op])\n",
    "    loss_op = toxic_loss_op\n",
    "    \n",
    "    #toxic_loss_summary_op = tf.summary.scalar(\"toxic_loss_op\", toxic_loss_op)\n",
    "    #severe_toxic_loss_op_summary = tf.summary.scalar(\"severe_toxic_loss_op\", severe_toxic_loss_op)\n",
    "    #obscene_loss_op_summary = tf.summary.scalar(\"obscene_loss_op\", obscene_loss_op)\n",
    "    #threat_loss_op_summary = tf.summary.scalar(\"threat_loss_op\", threat_loss_op)\n",
    "    #insult_loss_op_summary = tf.summary.scalar(\"insult_loss_op\", insult_loss_op)\n",
    "    #identity_hate_loss_op_summary = tf.summary.scalar(\"identity_hate_loss_op\", identity_hate_loss_op)\n",
    "\n",
    "    #loss_op_summary = tf.summary.scalar(\"loss_op\", loss_op)\n",
    "\n",
    "#file_writer = tf.summary.FileWriter(LOG_DIRECTORY, tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## train parameters\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 1\n",
    "NUM_DISPLAY_STEPS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## 9. define train_op\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "\n",
    "    global_step = tf.Variable(0, name = \"global_step\", trainable = False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss_op)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step)\n",
    "    \n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.353208, development_loss 0.531299, step 1, took 0.068475\n",
      "train_loss 0.783387, development_loss 0.524968, step 2, took 3.4726\n",
      "train_loss 0.396906, development_loss 0.516493, step 3, took 3.39062\n",
      "train_loss 0.399177, development_loss 0.507225, step 4, took 3.38571\n",
      "train_loss 0.340869, development_loss 0.49754, step 5, took 3.39116\n",
      "train_loss 0.354436, development_loss 0.488027, step 6, took 3.44742\n",
      "train_loss 0.382612, development_loss 0.47846, step 7, took 3.41393\n",
      "train_loss 0.431709, development_loss 0.468877, step 8, took 3.35485\n",
      "train_loss 0.888589, development_loss 0.461235, step 9, took 3.4642\n",
      "train_loss 0.34681, development_loss 0.453595, step 10, took 3.40583\n",
      "train_loss 0.397027, development_loss 0.446271, step 11, took 3.36699\n",
      "train_loss 0.400394, development_loss 0.439066, step 12, took 3.5083\n",
      "train_loss 0.436558, development_loss 0.431869, step 13, took 3.4356\n",
      "train_loss 0.217016, development_loss 0.424947, step 14, took 3.40635\n",
      "train_loss 0.592164, development_loss 0.418234, step 15, took 3.3625\n",
      "train_loss 1.02451, development_loss 0.412765, step 16, took 3.38928\n",
      "train_loss 0.252803, development_loss 0.40743, step 17, took 3.40729\n",
      "train_loss 0.367137, development_loss 0.402204, step 18, took 3.40648\n",
      "train_loss 0.869171, development_loss 0.398181, step 19, took 3.39157\n",
      "train_loss 0.904438, development_loss 0.395203, step 20, took 3.36869\n",
      "train_loss 0.384166, development_loss 0.392048, step 21, took 3.3417\n",
      "train_loss 0.237658, development_loss 0.388673, step 22, took 3.42593\n",
      "train_loss 0.844294, development_loss 0.386019, step 23, took 3.41007\n",
      "train_loss 0.271999, development_loss 0.383192, step 24, took 3.35992\n",
      "train_loss 0.339648, development_loss 0.380132, step 25, took 3.40439\n",
      "train_loss 0.361031, development_loss 0.376976, step 26, took 3.38547\n",
      "train_loss 0.234919, development_loss 0.373758, step 27, took 3.41484\n",
      "train_loss 0.186483, development_loss 0.37056, step 28, took 3.41714\n",
      "train_loss 0.238413, development_loss 0.367397, step 29, took 3.42431\n",
      "train_loss 0.187616, development_loss 0.364278, step 30, took 3.34707\n",
      "train_loss 0.199078, development_loss 0.361233, step 31, took 3.36008\n",
      "train_loss 0.209415, development_loss 0.358229, step 32, took 3.41288\n",
      "train_loss 0.23167, development_loss 0.35527, step 33, took 3.441\n",
      "train_loss 0.310734, development_loss 0.352328, step 34, took 3.32701\n",
      "train_loss 0.18326, development_loss 0.349528, step 35, took 3.43931\n",
      "train_loss 0.85111, development_loss 0.347444, step 36, took 3.42349\n",
      "train_loss 1.34598, development_loss 0.346152, step 37, took 3.40852\n",
      "train_loss 0.290469, development_loss 0.344666, step 38, took 3.40794\n",
      "train_loss 0.125171, development_loss 0.343187, step 39, took 3.36682\n",
      "train_loss 0.292102, development_loss 0.341651, step 40, took 3.41364\n",
      "train_loss 0.166477, development_loss 0.340121, step 41, took 3.42423\n",
      "train_loss 0.257816, development_loss 0.338539, step 42, took 3.44271\n",
      "train_loss 0.175448, development_loss 0.336964, step 43, took 3.46694\n",
      "train_loss 0.147175, development_loss 0.335456, step 44, took 3.57911\n",
      "train_loss 0.130644, development_loss 0.334005, step 45, took 3.49365\n",
      "train_loss 0.227543, development_loss 0.332581, step 46, took 3.54409\n",
      "train_loss 0.159192, development_loss 0.331218, step 47, took 3.54774\n",
      "train_loss 0.180093, development_loss 0.329906, step 48, took 3.5317\n",
      "train_loss 0.164133, development_loss 0.328656, step 49, took 3.47872\n",
      "train_loss 0.163755, development_loss 0.327485, step 50, took 3.55037\n",
      "train_loss 1.13867, development_loss 0.326723, step 51, took 3.54393\n",
      "train_loss 0.118425, development_loss 0.325976, step 52, took 3.51744\n",
      "train_loss 0.11789, development_loss 0.325263, step 53, took 3.52979\n",
      "train_loss 0.118591, development_loss 0.324564, step 54, took 3.49497\n",
      "train_loss 0.051399, development_loss 0.323925, step 55, took 3.52753\n",
      "train_loss 0.0761767, development_loss 0.323328, step 56, took 3.53523\n",
      "train_loss 0.128097, development_loss 0.322744, step 57, took 3.5431\n",
      "train_loss 0.0946249, development_loss 0.322208, step 58, took 3.51531\n",
      "train_loss 0.174475, development_loss 0.321686, step 59, took 3.51113\n",
      "train_loss 0.144126, development_loss 0.321191, step 60, took 3.82476\n",
      "train_loss 0.151626, development_loss 0.320728, step 61, took 3.86662\n",
      "train_loss 0.124131, development_loss 0.320312, step 62, took 3.87191\n",
      "train_loss 0.0663883, development_loss 0.319958, step 63, took 3.85581\n",
      "train_loss 0.12665, development_loss 0.319654, step 64, took 3.87907\n",
      "train_loss 0.0877624, development_loss 0.3194, step 65, took 3.85692\n",
      "train_loss 0.15292, development_loss 0.319182, step 66, took 3.86301\n",
      "train_loss 1.35573, development_loss 0.319012, step 67, took 3.86215\n",
      "train_loss 1.07104, development_loss 0.318877, step 68, took 3.86229\n",
      "train_loss 0.102378, development_loss 0.318749, step 69, took 3.86068\n",
      "train_loss 0.0849358, development_loss 0.318627, step 70, took 3.86128\n",
      "train_loss 1.15207, development_loss 0.318516, step 71, took 3.86598\n",
      "train_loss 0.0774853, development_loss 0.318403, step 72, took 3.86144\n",
      "train_loss 0.117088, development_loss 0.318286, step 73, took 3.86231\n",
      "train_loss 0.11062, development_loss 0.31817, step 74, took 3.88083\n",
      "train_loss 0.156296, development_loss 0.318055, step 75, took 3.85536\n",
      "train_loss 0.114144, development_loss 0.317944, step 76, took 3.86699\n",
      "train_loss 0.0860646, development_loss 0.317841, step 77, took 3.85851\n",
      "train_loss 0.132345, development_loss 0.317739, step 78, took 3.88194\n",
      "train_loss 0.0663151, development_loss 0.317657, step 79, took 3.86786\n",
      "train_loss 1.197, development_loss 0.317571, step 80, took 3.86962\n",
      "train_loss 0.0983589, development_loss 0.317495, step 81, took 3.86067\n",
      "train_loss 0.096124, development_loss 0.317425, step 82, took 3.88731\n",
      "train_loss 0.145545, development_loss 0.317356, step 83, took 3.88091\n",
      "train_loss 0.0847273, development_loss 0.317292, step 84, took 3.87796\n",
      "train_loss 0.115759, development_loss 0.317245, step 85, took 3.87933\n",
      "train_loss 0.355024, development_loss 0.317221, step 86, took 3.86537\n",
      "train_loss 0.0635762, development_loss 0.317218, step 87, took 3.68507\n",
      "train_loss 1.04043, development_loss 0.317182, step 88, took 3.40517\n",
      "train_loss 0.0903216, development_loss 0.317159, step 89, took 3.38209\n",
      "train_loss 0.153429, development_loss 0.317156, step 90, took 3.46535\n",
      "train_loss 0.0891141, development_loss 0.317166, step 91, took 3.38449\n",
      "train_loss 0.0571018, development_loss 0.317192, step 92, took 3.41278\n",
      "train_loss 0.184102, development_loss 0.317246, step 93, took 3.37933\n",
      "train_loss 0.0731884, development_loss 0.31732, step 94, took 3.42387\n",
      "train_loss 1.67301, development_loss 0.317238, step 95, took 3.40137\n",
      "train_loss 0.0635654, development_loss 0.317181, step 96, took 3.38309\n",
      "train_loss 0.0801285, development_loss 0.317145, step 97, took 3.41672\n",
      "train_loss 0.102187, development_loss 0.31713, step 98, took 3.40238\n",
      "train_loss 0.0986298, development_loss 0.317136, step 99, took 3.76756\n",
      "train_loss 0.0497158, development_loss 0.317157, step 100, took 4.08194\n",
      "train_loss 1.17522, development_loss 0.317092, step 101, took 4.0421\n",
      "train_loss 1.18903, development_loss 0.316958, step 102, took 4.03673\n",
      "train_loss 0.0638753, development_loss 0.31686, step 103, took 4.04543\n",
      "train_loss 1.13769, development_loss 0.316723, step 104, took 4.0904\n",
      "train_loss 1.32675, development_loss 0.316558, step 105, took 4.11453\n",
      "train_loss 1.4993, development_loss 0.316447, step 106, took 4.06735\n",
      "train_loss 0.0909213, development_loss 0.316442, step 107, took 4.10607\n",
      "train_loss 0.0594981, development_loss 0.316507, step 108, took 4.0726\n",
      "train_loss 0.961919, development_loss 0.31667, step 109, took 4.06153\n",
      "train_loss 0.134907, development_loss 0.316873, step 110, took 4.08334\n",
      "train_loss 0.0716303, development_loss 0.31709, step 111, took 4.09323\n",
      "train_loss 1.45217, development_loss 0.317495, step 112, took 4.06134\n",
      "train_loss 0.0690824, development_loss 0.317915, step 113, took 4.05122\n",
      "train_loss 0.113294, development_loss 0.31832, step 114, took 4.0744\n",
      "train_loss 1.13474, development_loss 0.318914, step 115, took 4.03277\n",
      "train_loss 2.43761, development_loss 0.319906, step 116, took 4.12098\n",
      "train_loss 0.207448, development_loss 0.320876, step 117, took 4.05568\n",
      "train_loss 0.594701, development_loss 0.321921, step 118, took 4.08638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.157385, development_loss 0.322877, step 119, took 4.09421\n",
      "train_loss 0.78329, development_loss 0.324036, step 120, took 4.07587\n",
      "train_loss 0.103062, development_loss 0.325087, step 121, took 4.08535\n",
      "train_loss 0.075869, development_loss 0.326009, step 122, took 4.02704\n",
      "train_loss 0.139356, development_loss 0.326745, step 123, took 4.10466\n",
      "train_loss 0.102967, development_loss 0.327323, step 124, took 4.06037\n",
      "train_loss 0.0962384, development_loss 0.327753, step 125, took 4.09651\n",
      "train_loss 0.113105, development_loss 0.328032, step 126, took 4.04493\n",
      "train_loss 0.214304, development_loss 0.328149, step 127, took 4.02875\n",
      "train_loss 0.110691, development_loss 0.328138, step 128, took 4.04864\n",
      "train_loss 0.0968806, development_loss 0.328016, step 129, took 4.04356\n",
      "train_loss 1.18191, development_loss 0.328408, step 130, took 4.08666\n",
      "train_loss 0.135343, development_loss 0.328629, step 131, took 4.06604\n",
      "train_loss 0.150517, development_loss 0.328675, step 132, took 4.04341\n",
      "train_loss 0.948992, development_loss 0.328981, step 133, took 4.07499\n",
      "train_loss 0.287162, development_loss 0.32909, step 134, took 4.04362\n",
      "train_loss 0.137842, development_loss 0.329051, step 135, took 4.109\n",
      "train_loss 0.197368, development_loss 0.328825, step 136, took 4.02342\n",
      "train_loss 0.110173, development_loss 0.328481, step 137, took 4.07296\n",
      "train_loss 1.24812, development_loss 0.328551, step 138, took 4.06636\n",
      "train_loss 1.26123, development_loss 0.328966, step 139, took 4.06366\n",
      "train_loss 0.115314, development_loss 0.329206, step 140, took 4.10362\n",
      "train_loss 0.195577, development_loss 0.329232, step 141, took 4.12572\n",
      "train_loss 0.0909019, development_loss 0.329137, step 142, took 4.11924\n",
      "train_loss 0.146828, development_loss 0.328914, step 143, took 4.03018\n",
      "train_loss 0.151211, development_loss 0.328564, step 144, took 4.07036\n",
      "train_loss 0.230158, development_loss 0.328062, step 145, took 4.06855\n",
      "train_loss 0.133869, development_loss 0.327478, step 146, took 4.04724\n",
      "train_loss 0.132654, development_loss 0.32683, step 147, took 4.0543\n",
      "train_loss 0.0807701, development_loss 0.326175, step 148, took 4.04617\n",
      "train_loss 1.12705, development_loss 0.325975, step 149, took 4.05722\n",
      "train_loss 0.157819, development_loss 0.325643, step 150, took 4.10806\n",
      "train_loss 0.169671, development_loss 0.325211, step 151, took 4.11714\n",
      "train_loss 0.143773, development_loss 0.324716, step 152, took 4.12242\n",
      "train_loss 0.25897, development_loss 0.324153, step 153, took 4.0782\n",
      "train_loss 0.0812756, development_loss 0.323592, step 154, took 4.08114\n",
      "train_loss 0.196589, development_loss 0.322975, step 155, took 4.12532\n",
      "train_loss 0.0896241, development_loss 0.322366, step 156, took 4.09196\n",
      "train_loss 0.15036, development_loss 0.321761, step 157, took 4.05584\n",
      "train_loss 0.0686774, development_loss 0.321184, step 158, took 4.102\n",
      "train_loss 1.16252, development_loss 0.320951, step 159, took 4.1\n",
      "train_loss 0.17853, development_loss 0.32066, step 160, took 4.06171\n",
      "train_loss 0.154776, development_loss 0.320319, step 161, took 4.08976\n",
      "train_loss 0.121322, development_loss 0.319951, step 162, took 4.04504\n",
      "train_loss 0.983587, development_loss 0.319795, step 163, took 4.11452\n",
      "train_loss 0.123108, development_loss 0.319582, step 164, took 4.11009\n",
      "train_loss 1.26345, development_loss 0.319663, step 165, took 4.09596\n",
      "train_loss 0.108217, development_loss 0.31967, step 166, took 4.08997\n",
      "train_loss 1.18701, development_loss 0.31989, step 167, took 4.05898\n",
      "train_loss 0.0826266, development_loss 0.320043, step 168, took 4.06037\n",
      "train_loss 0.170566, development_loss 0.320082, step 169, took 4.06555\n",
      "train_loss 1.26079, development_loss 0.320379, step 170, took 4.08628\n",
      "train_loss 0.201172, development_loss 0.320571, step 171, took 4.10404\n",
      "train_loss 0.135448, development_loss 0.320665, step 172, took 4.05172\n",
      "train_loss 0.179768, development_loss 0.320652, step 173, took 4.12136\n",
      "train_loss 0.118819, development_loss 0.320559, step 174, took 4.05066\n",
      "train_loss 0.128622, development_loss 0.320396, step 175, took 4.03451\n",
      "train_loss 0.155314, development_loss 0.320176, step 176, took 4.04017\n",
      "train_loss 0.637077, development_loss 0.320079, step 177, took 4.05858\n",
      "train_loss 0.110023, development_loss 0.319927, step 178, took 4.14941\n",
      "train_loss 1.39282, development_loss 0.320104, step 179, took 4.15684\n",
      "train_loss 0.128069, development_loss 0.320186, step 180, took 4.11486\n",
      "train_loss 0.100183, development_loss 0.320194, step 181, took 4.04588\n",
      "train_loss 0.0772691, development_loss 0.320143, step 182, took 4.08766\n",
      "train_loss 0.208244, development_loss 0.320005, step 183, took 4.04888\n",
      "train_loss 0.142637, development_loss 0.319802, step 184, took 4.08686\n",
      "train_loss 0.195484, development_loss 0.319538, step 185, took 4.07228\n",
      "train_loss 0.0761542, development_loss 0.319256, step 186, took 4.10637\n",
      "train_loss 1.02588, development_loss 0.319211, step 187, took 4.03725\n",
      "train_loss 0.120647, development_loss 0.319119, step 188, took 4.10191\n",
      "train_loss 0.117, development_loss 0.318976, step 189, took 4.12801\n",
      "train_loss 0.0900372, development_loss 0.318804, step 190, took 4.10406\n",
      "train_loss 0.0570974, development_loss 0.318615, step 191, took 4.0415\n",
      "train_loss 0.192516, development_loss 0.318391, step 192, took 4.09593\n",
      "train_loss 0.184893, development_loss 0.318128, step 193, took 4.05609\n",
      "train_loss 0.0969224, development_loss 0.317855, step 194, took 4.10698\n",
      "train_loss 1.58023, development_loss 0.317812, step 195, took 4.02726\n",
      "train_loss 0.119675, development_loss 0.317731, step 196, took 4.05951\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "## session\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    start_of_step_time = default_timer()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "        idx_batches = make_idx_batches(x_comment_text_train, BATCH_SIZE)\n",
    "    \n",
    "        for idx_batch in idx_batches:\n",
    "\n",
    "            x_comment_text_batch = x_comment_text_train[idx_batch]\n",
    "            \n",
    "            toxic_labels_batch = toxic_labels_train[idx_batch]\n",
    "            #severe_toxic_labels_batch = severe_toxic_labels_train[idx_batch]\n",
    "            #obscene_labels_batch = obscene_labels_train[idx_batch]\n",
    "            #threat_labels_batch = threat_labels_train[idx_batch]\n",
    "            #insult_labels_batch = insult_labels_train[idx_batch]\n",
    "            #identity_hate_labels_batch = identity_hate_labels_train[idx_batch]\n",
    "        \n",
    "            # train\n",
    "            train_feed_dict = {\n",
    "                comment_text_placeholder: x_comment_text_batch,\n",
    "                toxic_placeholder: toxic_labels_batch,\n",
    "             #   severe_toxic_placeholder: severe_toxic_labels_batch, \n",
    "             #   obscene_placeholder: obscene_labels_batch,\n",
    "             #   threat_placeholder: threat_labels_batch,\n",
    "             #   insult_placeholder: insult_labels_batch,\n",
    "             #   identity_hate_placeholder: identity_hate_labels_batch\n",
    "            }\n",
    "            \n",
    "            _, train_loss = session.run([train_op, loss_op], feed_dict = train_feed_dict)\n",
    "                                \n",
    "            current_step = tf.train.global_step(session, global_step)\n",
    "            \n",
    "            if current_step % NUM_DISPLAY_STEPS == 0:\n",
    "                \n",
    "                took = default_timer() - start_of_step_time\n",
    "                start_of_step_time = default_timer()\n",
    "\n",
    "                development_feed_dict = {\n",
    "                    comment_text_placeholder: x_comment_text_development,\n",
    "                    toxic_placeholder: toxic_labels_development,\n",
    "                #    severe_toxic_placeholder: severe_toxic_labels_development, \n",
    "                #    obscene_placeholder: obscene_labels_development,\n",
    "                #    threat_placeholder: threat_labels_development,\n",
    "                #    insult_placeholder: insult_labels_development,\n",
    "                #    identity_hate_placeholder: identity_hate_labels_development\n",
    "                }\n",
    "\n",
    "                development_loss = session.run(fetches = loss_op, feed_dict = development_feed_dict)\n",
    "                print(\"train_loss {:g}, development_loss {:g}, step {:g}, took {:g}\".format(\n",
    "                    train_loss, development_loss, current_step, took))\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
