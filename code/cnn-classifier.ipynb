{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/agrigorev/tensorflow-starter-conv1d-embeddings-0-442-lb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "response_df = pd.read_pickle(\"../output/response_df.pkl\")\n",
    "train_df = pd.read_pickle(\"../output/train_df.pkl\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "WHITESPACE_RE = re.compile(r'\\s+')\n",
    "TOKENIZER_RE = re.compile(r\"[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\'\\w\\-]+\", re.UNICODE)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = WHITESPACE_RE.sub(' ', text)\n",
    "    text = TOKENIZER_RE.findall(text)\n",
    "    return text\n",
    "\n",
    "comment_text_series = train_df['comment_text'].str.lower()\n",
    "comment_text_series_split = comment_text_series.apply(tokenize)\n",
    "comment_text_list = comment_text_series_split.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [element for list_ in comment_text_list for element in list_]\n",
    "tokens_with_counts = Counter(tokens)\n",
    "\n",
    "unique_tokens = [k for k, v in tokens_with_counts.items()]\n",
    "unique_tokens_with_idx = {token:(i+1) for i, token in enumerate(unique_tokens)}\n",
    "\n",
    "max_list_length = max([len(token_list) for token_list in comment_text_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46, 47, 48, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "def token_list_to_idx(token_list):\n",
    "    token_idx = [unique_tokens_with_idx[token] for token in token_list if token in unique_tokens_with_idx]\n",
    "    token_idx_pad = token_idx + [0] * (max_list_length - len(token_idx))\n",
    "    return token_idx_pad\n",
    "\n",
    "comment_text_idx_list = [token_list_to_idx(token_list) for token_list in comment_text_list]\n",
    "\n",
    "comment_text_idx = np.array(comment_text_idx_list) # X_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 1403)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_text_embeddings_dim = 32\n",
    "comment_text_seq_len = comment_text_idx.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed(ids, size, dim):\n",
    "    std = np.sqrt(2 / dim)\n",
    "    params = tf.Variable(tf.random_uniform([size, dim], -std, std))\n",
    "    lookup = tf.nn.embedding_lookup(params = params, ids = ids)\n",
    "    return lookup\n",
    "\n",
    "def make_idx_batch(idx, batch_size):\n",
    "    res = []\n",
    "    for i in range(0, len(idx), batch_size):\n",
    "        res.append(idx[i:(i+batch_size)])\n",
    "    return res\n",
    "\n",
    "def conv1d(inputs, num_filters, filter_size, padding='same'):\n",
    "    he_std = np.sqrt(2 / (filter_size * num_filters))\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs, filters=num_filters, padding=padding,\n",
    "        kernel_size=filter_size,\n",
    "        activation=tf.nn.relu, \n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=he_std))\n",
    "    return out\n",
    "\n",
    "def dense(X, size, reg=0.0, activation=None):\n",
    "    he_std = np.sqrt(2 / int(X.shape[1]))\n",
    "    out = tf.layers.dense(X, units=size, activation=activation, \n",
    "                     kernel_initializer=tf.random_normal_initializer(stddev=he_std),\n",
    "                     kernel_regularizer=tf.contrib.layers.l2_regularizer(reg))\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "comment_text_placeholder = tf.placeholder(tf.int32, shape = (None, comment_text_seq_len))\n",
    "toxic_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, 1))\n",
    "severe_toxic_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, 1))\n",
    "obscene_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, 1))\n",
    "threat_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, 1))\n",
    "insult_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, 1))\n",
    "identity_hate_placeholder = tf.placeholder(dtype = tf.float32, shape = (None, 1))\n",
    "\n",
    "lr_placeholder = tf.placeholder(tf.float32, shape = ())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_text_embedding = embed(comment_text_placeholder, len(unique_tokens), comment_text_embeddings_dim)\n",
    "comment_text_embedding = conv1d(comment_text_embedding, num_filters=10, filter_size=3)\n",
    "comment_text_embedding = tf.layers.dropout(comment_text_embedding, rate=0.5)\n",
    "comment_text_embedding = tf.contrib.layers.flatten(comment_text_embedding)\n",
    "print(comment_text_embedding.shape)\n",
    "\n",
    "out = dense(comment_text_embedding, 100, activation = tf.nn.relu)\n",
    "out = tf.layers.dropout(out, rate=0.5)\n",
    "\n",
    "predictions = dense(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "log_loss(labels = , predictions)\n",
    "\n",
    "\n",
    "def cl(labels, predictions):\n",
    "    \n",
    "\n",
    "loss = tf.losses.mean_squared_error(place_y, out)\n",
    "rmse = tf.sqrt(loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate=place_lr)\n",
    "train_step = opt.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/blogs/textclassification/txtcls1/trainer/model.py\n",
    "\n",
    "\n",
    "table = lookup.index_table_from_file(vocabulary_file=WORD_VOCAB_FILE, num_oov_buckets=1, default_value=-1)\n",
    "\n",
    "# string operations\n",
    "titles = tf.squeeze(comment_text_series_split_tf, [1])\n",
    "words = tf.string_split(titles)\n",
    "densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
    "numbers = table.lookup(densewords)\n",
    "padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
    "padded = tf.pad(numbers, padding)\n",
    "sliced = tf.slice(padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
    "print('words_sliced={}'.format(words))  # (?, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "import tensorflow as tf\n",
    " \n",
    "################################################################################\n",
    "################################################################################\n",
    "##\n",
    "\n",
    "comment_texts = tf.constant(comment_text_series_split_lists)\n",
    "table = tf.contrib.lookup.index_to_string_table_from_tensor(mapping = comment_texts)\n",
    "comment_texts_values = table.lookup(indices)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.tables_initializer().run()\n",
    "    print(values.eval())\n",
    "    \n",
    "MAX_DOCUMENT_LENGTH = comment_text_series_split.apply(len).max()\n",
    "padding = tf.constant([[0,0], [0, MAX_DOCUMENT_LENGTH]])\n",
    "padded = tf.pad(comment_texts_values, padding)\n",
    "sliced = tf.slice(padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
    "#numbers = table.lookup(comment_words_dense)\n",
    "\n",
    "#from tensorflow.contrib import lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "## \n",
    "\n",
    "from tensorflow.contrib.learn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 5  \n",
    "PADWORD = 'ZYXW'\n",
    "\n",
    "# create vocabulary\n",
    "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "vocab_processor.fit(lines)\n",
    "\n",
    "with gfile.Open('vocab.tsv', 'wb') as f:\n",
    "    f.write(\"{}\\n\".format(PADWORD))\n",
    "    for word, index in vocab_processor.vocabulary_._mapping.iteritems():\n",
    "      f.write(\"{}\\n\".format(word))\n",
    "N_WORDS = len(vocab_processor.vocabulary_)\n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_batches(seq, step):\n",
    "    n = len(seq)\n",
    "    res = []\n",
    "    for i in range(0, n, step):\n",
    "        res.append(seq[i:i+step])\n",
    "    return res\n",
    "\n",
    "def conv1d(inputs, num_filters, filter_size, padding='same'):\n",
    "    he_std = np.sqrt(2 / (filter_size * num_filters))\n",
    "    out = tf.layers.conv1d(\n",
    "        inputs=inputs, filters=num_filters, padding=padding,\n",
    "        kernel_size=filter_size,\n",
    "        activation=tf.nn.relu, \n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=he_std))\n",
    "    return out\n",
    "\n",
    "def dense(X, size, reg=0.0, activation=None):\n",
    "    he_std = np.sqrt(2 / int(X.shape[1]))\n",
    "    out = tf.layers.dense(X, units=size, activation=activation, \n",
    "                     kernel_initializer=tf.random_normal_initializer(stddev=he_std),\n",
    "                     kernel_regularizer=tf.contrib.layers.l2_regularizer(reg))\n",
    "    return out\n",
    "\n",
    "def embed(inputs, size, dim):\n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std))\n",
    "    lookup = tf.nn.embedding_lookup(emb, inputs)\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN model parameters\n",
    "EMBEDDING_SIZE = 10\n",
    "WINDOW_SIZE = EMBEDDING_SIZE\n",
    "STRIDE = int(WINDOW_SIZE/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
